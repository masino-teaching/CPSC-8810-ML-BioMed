{"cells":[{"cell_type":"markdown","metadata":{"id":"UoLJSZUvuY6Y"},"source":["CPSC 8810 Machine Learning for Biomedical Applications\n","\n","# Assignment 3 - Classifiers with Image and Text Data\n","# Molecular Similarity Dataset\n","In this assignment, you are asked to use the `molecular-similarity` dataset to create classification models that predict whether or not a pair of molecules are similar. The available data include images of the 2D molecular structure and the [Simplified molecular-input line-entry system (SMILES)](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) string representation of the molecular formula. The data are arranged in pairs where each pair contains the image and SMILES formula for two molecules and the similarity score (the fraction of 20 human expert raters that rated the molecules as similar).\n","\n","The data are contained in the course repository in the _assignments/source_data/molecular-similarity_ folder. The folder contains a _data.csv_ file in which each row contains represents the information for a pair of molecules `(a,b)`. The SMILES formula for both molecules and the corresponding names of the 2D image files are given in the row. The `frac_similar` column is the similarity score for the pair of molecules. Detailed information on the data set can be found [here](https://www.kaggle.com/datasets/tanvirnwu/molecular-similarity-prediction-dataset) and in the related journal article [Molecular Similarity Perception Based on Machine-Learning Models](https://www.mdpi.com/1422-0067/23/11/6114)\n","\n","__Please read through the notebook and follow the instructions for each of the 6 problems. For extra credit, you may also do Prblem 7__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uD1XMsT0ubfz"},"outputs":[],"source":["# Google Colab setup\n","# mount the google drive - this is necessary to access supporting src\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"93GvrNqaucqf"},"outputs":[],"source":["# install any packages not found in the Colab environment\n","!pip install lightning\n","!pip install 'portalocker>=2.0.0'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Swt3bHRuY6Z"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import pytorch_lightning as pl\n","import torchtext\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchvision.transforms import ToTensor\n","from torchvision.utils import make_grid\n","from torchvision import transforms, datasets, models\n","from torchvision.io import read_image\n","import torchvision.transforms.functional as F\n","from torch.utils.data import DataLoader\n","from torchtext.datasets import AG_NEWS\n","from torchtext.functional import to_tensor\n","import lightning as L\n","from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n","from lightning.pytorch import seed_everything\n","import lightning.pytorch.trainer as trainer\n","import torchmetrics as TM\n","import torchmetrics as TM\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import classification_report\n","\n","# local project imports\n","import sys\n","sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/CPSC-8810-ML-BioMed/src\")\n","from torchvision_utils import show"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqyxHfDfuY6Z"},"outputs":[],"source":["dir_dataroot = \"/content/drive/MyDrive/Colab Notebooks/CPSC-8810-ML-BioMed/assignments/source_data\"\n","\n","dir_lightning = \"/content/drive/MyDrive/Colab Notebooks/CPSC-8810-ML-BioMed/lightning\"\n","\n","rs = 123456 # random seed for everything"]},{"cell_type":"markdown","metadata":{"id":"KkDQjmjpuY6Z"},"source":["# Data Module\n","As in the practicums, it is useful to create a PyTorch Lightning data module to handle loading and processing data for batch training with our PyTorch modules. In this assignment, we are working with mulitmodal data that includes both images and text as possible input to the model. The `MolecularSimilarityDataModule` in the code cell below, will handle all data loading and should __NOT__ be modified. However, it is important that you review the code to understand how the data module works. Importantly, the data module takes the following inputs to its constructor (`__init__`):\n","- `data_dir` - the directory containing the _data.csv_ file and the _images-2D-#x#_ direcotry where _#_ is the image pixel dimension (e.g., 128)\n","- `image_dim` - the pixel dimension (assumed to be square) of the 2D molecular images\n","- `embedding_dim` - the token embedding dimension when using the SMILES text input\n","- `modalities` - determines which data to use for input: 0 - SMILES text only, 1 - 2D images only, 2 - both text and images\n","- `class_threshold_map` - a dictionary in the form {c1:(0, x1), c2:(x1, x2), ..., cK:(x_{k-1},1.01)} that defines the frac_similarity bins that define the classes [c1, c2, ..., ck]\n","\n","The data module class creates train, validation, and test dataloaders using the `MolecularSimilarityDataSet` class that extends the PyTorch `Dataset` class. This class derives the sample set size (the number of scored pairs of molecules) from the _data.csv_ file.\n","\n","In the problems below, we will use this data module class to support creating a text only model, an image only model and (for extra credit) and multimodal model that uses both the text and image inputs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vTyrt1aEuY6Z"},"outputs":[],"source":["####################################################################################################\n","# DO NOT CHANGE THIS CELL\n","####################################################################################################\n","class MolecularSimilarityDataSet(torch.utils.data.Dataset):\n","    def __init__(self, data_dir):\n","        self.data_dir = data_dir\n","        self.samples = self.load_data()\n","        self.len = len(self.samples)\n","\n","    def load_data(self):\n","        samples = []\n","        with open(os.path.join(self.data_dir, 'data.csv'), 'r') as f:\n","            # skip the header\n","            next(f)\n","            for line in f:\n","                data = line.strip().split(',')\n","                samples.append(data[1:])\n","        return samples\n","\n","    def __len__(self):\n","        return self.len\n","\n","    def __getitem__(self, idx):\n","        # each sample is a list: [smilea, smileb, imagea, imageb, frac_similar]\n","        return self.samples[idx]\n","\n","class MolecularSimilarityDataModule(L.LightningDataModule):\n","    def __init__(self, data_dir, class_threshold_map, train_fraction=0.875, val_fraction=0.145, batch_size=10,\n","                 embedding_dim=8, image_dim = 128, modalities='2', class_name_map=None):\n","        \"\"\"\n","        Args:\n","            data_dir: str - path to the data directory\n","            val_fraction: float - fraction of the data to use for validation\n","            batch_size: int - batch size\n","            embedding_dim: int - dimension of the embeddings\n","            modalities: str - 0: text only, 1: image only, 2: text and image\n","        \"\"\"\n","        super().__init__()\n","        self.data_dir = data_dir\n","        self.val_fraction = val_fraction\n","        self.train_fraction = train_fraction\n","        self.batch_size = batch_size\n","        self.embedding_dim = embedding_dim\n","        self.modalities = modalities\n","        self.vocab = None\n","        self.dataset = None\n","        self.max_tokens = None\n","        self.class_threshold_map = class_threshold_map\n","        self.class_name_map = class_name_map\n","        self.image_dim = image_dim\n","\n","    def tokenize(self, text):\n","        s = set(text)\n","        if ' ' in s:\n","            s.remove(' ')\n","        return list(s)\n","\n","    def label_pipeline(self, frac_similar):\n","        # bin the samples based on the fraction similarity thresholds specified in the class_map\n","        # {0:(0, 0.25), 1:(0.25, 0.75), 2:(0.75,1.01)}\n","        for k, v in self.class_threshold_map.items():\n","            lb = v[0]\n","            rb = v[1]\n","            if frac_similar>=lb and frac_similar < rb:\n","                return k\n","\n","    def max_tokens_in(self, data_iterable):\n","        if self.vocab is None:\n","            self.build_vocab(data_iterable)\n","        text_to_tokens = lambda x: self.vocab(self.tokenize(x))\n","        max_tokens = 0\n","        #each sample is a list: [smilea, smileb, imagea, imageb, frac_similar]\n","        for sample in data_iterable:\n","            smilea = sample[0]\n","            smileb = sample[1]\n","            l1 = len(text_to_tokens(smilea))\n","            l2 = len(text_to_tokens(smileb))\n","            l = max(l1, l2)\n","            if l > max_tokens:\n","                max_tokens = l\n","        return max_tokens+2 # add 2 for start and end tokens\n","\n","    def max_sample_length(self):\n","        return self.max_tokens\n","\n","    def build_vocab(self, data_iter):\n","        def yield_tokens(data_iter):\n","            for sample in data_iter:\n","                smilea = set(sample[0])\n","                smileb = set(sample[1])\n","                smileab = smilea.union(smileb)\n","                if ' ' in smileab:\n","                    smileab.remove(' ')\n","                yield smileab\n","\n","        self.vocab = build_vocab_from_iterator(yield_tokens(data_iter), specials=['<unk>', '<pad>', '<end>', '<start>'])\n","        self.vocab.set_default_index(self.vocab['<unk>'])\n","        self.padding_idx = self.vocab['<pad>']\n","        self.end_idx = self.vocab['<end>']\n","        self.start_idx = self.vocab['<start>']\n","        return self.vocab\n","\n","    def collate_batch(self, batch):\n","        #each sample is a list: [smilea, smileb, imagea, imageb, frac_similar]\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        label_list = []\n","        if self.modalities == 0 or self.modalities == 2: # text only\n","            text_batch = torch.zeros(len(batch), 2*self.max_tokens, dtype=torch.long)\n","        if self.modalities == 1 or self.modalities == 2: # image only\n","            image_batch = torch.zeros(len(batch), 6, self.image_dim, self.image_dim)\n","            image_to_tensor = transforms.ToTensor()\n","\n","        cnt = 0\n","        for sample in batch:\n","            label_list.append(self.label_pipeline(float(sample[4])))\n","            if self.modalities == 0 or self.modalities == 2:\n","                text = [self.start_idx]\n","                text.extend(self.vocab(self.tokenize(sample[0])))\n","                if len(text) < self.max_tokens-1:\n","                    text.extend([self.padding_idx] * (self.max_tokens - len(text)-1))\n","                text.extend([self.end_idx, self.start_idx])\n","                text.extend(self.vocab(self.tokenize(sample[1])))\n","                if len(text) < 2*self.max_tokens-1:\n","                    text.extend([self.padding_idx] * (2*self.max_tokens - len(text)-1))\n","                text.append(self.end_idx)\n","                text_batch[cnt] = torch.tensor(text, dtype=torch.int64)\n","            if self.modalities == 1 or self.modalities == 2:\n","                imga = read_image(os.path.join(self.data_dir, f\"images-2D-{self.image_dim}x{self.image_dim}\", sample[2]))\n","                imgb = read_image(os.path.join(self.data_dir, f\"images-2D-{self.image_dim}x{self.image_dim}\", sample[3]))\n","                imga = image_to_tensor(imga.numpy())\n","                imga = imga.permute((1, 0, 2)).contiguous()\n","                imgb = image_to_tensor(imgb.numpy())\n","                imgb = imgb.permute((1,0,2)).contiguous()\n","                image_batch[cnt] = torch.cat((imga, imgb), 0)\n","            cnt += 1\n","        label_list = torch.tensor(label_list, dtype=torch.int64)\n","\n","        if self.modalities == 0:\n","            return text_batch.to(device), label_list.to(device)\n","        elif self.modalities == 1:\n","            return image_batch.to(device), label_list.to(device)\n","        else:\n","            return text_batch.to(device), image_batch.to(device), label_list.to(device)\n","\n","    def setup(self, stage=None):\n","        self.dataset = MolecularSimilarityDataSet(self.data_dir)\n","        if self.modalities == 0 or self.modalities == 2:\n","            # build the vocabulary\n","            self.build_vocab(self.dataset)\n","            # find max tokens\n","            self.max_tokens = self.max_tokens_in(self.dataset)\n","        n_data = len(self.dataset)\n","        n_train = int(self.train_fraction * n_data)\n","        n_val = int(self.val_fraction * n_train)\n","        n_train = n_train - n_val\n","        n_test = n_data - n_train - n_val\n","        train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(self.dataset, [n_train, n_val, n_test])\n","\n","        self._train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_batch)\n","        self._val_dataloader = DataLoader(val_dataset, batch_size=self.batch_size, collate_fn=self.collate_batch)\n","        self._test_dataloader = DataLoader(test_dataset, batch_size=self.batch_size, collate_fn=self.collate_batch)\n","\n","    def train_dataloader(self):\n","        return self._train_dataloader\n","\n","    def test_dataloader(self):\n","        return self._test_dataloader\n","\n","    def val_dataloader(self):\n","        return self._val_dataloader\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2Goco-WfuY6a"},"source":["# Text Input Molecular Similarity Classifier\n","\n","In the following section, you will build a classification model that predicts if a given pair of molecules are\n","1. Similar - defined as having a `frac_similar` score between 0.66 and 1.0\n","2. Uncertain - defined as having a `frac_similar` score between 0.33 and 0.66\n","3. Not Similar - defined as having a `frac_similar` score between 0 and 0.33\n","\n","In this first model, the predictions will be made using only the SMILES text representation of the molecular formulas. An example SMILES formula is _Cc1ccsc1-c1cccnc1_. You can view more examples in the _assignments/source_data/molecular-similarity/data.csv_ file. Importantly, these formulas are not composed of words like we saw in Practicum 7. Hence, we require a different tokeninzing strategy for inputting the formula to the model. We will tokenize the SMILES formula at the character level, that is each character in the formula, e.g., _C_, will be represented as a token in our vocabulary. This tokenization is handled by the `MolecularSimilarityDataModule`. The model we build below, will learn an embedding for each character."]},{"cell_type":"markdown","metadata":{"id":"kopLrHuKuY6a"},"source":["# Problem 1 (1 point)\n","\n","In the code cell below, create an instance of `MolecularSimilarityDataModule` that uses the text only modality. The arguments to constructor should be:\n","- data_dir = molecular_data_dir\n","- modalities = 0\n","- batch_size = 10\n","- class_threshold_map = ctm\n","- class_name_map = cnm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_GWpqHWVuY6a"},"outputs":[],"source":["seed_everything(rs)\n","ctm = {0:(0, 0.33), 1:(0.33, 0.66), 2:(0.66,1.01)}\n","cnm = {0:'Not Similar', 1:'Uncertain', 2:'Similar'}\n","\n","##### Problem 1 start your code here #####\n","dm = None\n","##### Problem 1 end your code here #####\n","\n","dm.setup()\n","print(len(dm.train_dataloader()))\n","print(len(dm.val_dataloader()))\n","print(len(dm.test_dataloader()))"]},{"cell_type":"markdown","metadata":{"id":"f0OEVQmiuY6a"},"source":["Now that we have the data module ready, we can build the molecular similarity classification model using only the SMILES text inputs. We will design the model following the same procedure that was presented in Practicum 7. Specifically, the model will include an _encoder_ and a _classifier_. The input will pass through the _encoder_ which will create a numerical representation of input SMILES text formula which will be passed to the _classifier_ to predict the class (similar, uncertain, or not similar). As in the practicum, the _encoder_ will be based on a Transformer and the _classifier_ will be a feed forward linear layer."]},{"cell_type":"markdown","metadata":{"id":"0s0AbqNHuY6a"},"source":["# Problem 2 (2 points)\n","In the code cell, complete the implementation of the `TextEncoder` class. In the `__init__` method, you will need to create the `embedding` layer and the `transformer_encoder` layer. You should use the input arguments to the `__init__` function to set the arguments in the `embedding` and `transformer_encoder`. In the forward method, you will need to pass the input, `x` through both layers.\n","\n","__HINT__: See Practicum 7."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F2QsAhJ4uY6a"},"outputs":[],"source":["class TextEncoder(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, num_heads,\n","                 num_transformer_layers=1, dim_feedforward=128, activation='relu', dropout=0.1):\n","        super().__init__()\n","        ##### Problem 2 start your code here #####\n","        self.embedding = None\n","        self.te_layer = None\n","        self.transformer_encoder = None\n","        ##### Problem 2 start your code here #####\n","\n","    def forward(self, x):\n","        ##### Problem 2 start your code here #####\n","\n","        ##### Problem 2 end your code here #####\n","        return x\n","\n","# Test the ImageEncoder with random input data\n","encoder = TextEncoder(len(dm.vocab), dm.embedding_dim, num_heads=2)\n","input_text = torch.randint(0, len(dm.vocab), (dm.batch_size, 2*dm.max_sample_length()))\n","print(\"input shape\",input_text.shape)\n","output_features = encoder(input_text)\n","print(\"Output shape:\", output_features.shape)"]},{"cell_type":"markdown","metadata":{"id":"Cru_lQHKuY6a"},"source":["# Problem 3 (2 points)\n","In the code cell, complete the implementation of the `TextClassifier` class. In the `__init__` method, you will need to create the `Flatten` layer and the `linear` feed forward layer. You should use the input arguments to the `__init__` function to set the arguments in the `linear` layer. In the forward method, you will need to pass the input, `x` through both layers.\n","\n","__HINT__: See Practicum 7."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pxnwBiOcuY6a"},"outputs":[],"source":["class TextClassifier(nn.Module):\n","    def __init__(self, embedding_dim, seq_length, num_classes):\n","        super().__init__()\n","        ##### Problem 3 start your code here #####\n","        self.flatten = None\n","        self.linear = None\n","        ##### Problem 3 start your code here #####\n","\n","    def forward(self, x):\n","        ##### Problem 3 start your code here #####\n","\n","        ##### Problem 3 end your code here #####\n","        return x\n","\n","# Test the module with random input data\n","classifier = TextClassifier(dm.embedding_dim, 2*dm.max_sample_length(), len(dm.class_name_map))\n","input_tensor = torch.randn(dm.batch_size, 2*dm.max_sample_length(), dm.embedding_dim)  # Batch size of 5, input tensor shape [5, 256, 64]\n","print(input_tensor.shape)\n","output = classifier(input_tensor)\n","print(\"Output shape:\", output.shape)  # Expected output shape: [4, 10]"]},{"cell_type":"markdown","metadata":{"id":"GfH9qz5GuY6a"},"source":["We are now ready to create our classification model. The code cell below extends the `LightningModule` from PyTorch Lightning to create the overall molecular similarity classification model. Note that this model takes an _encoder_ module and a _classifier_ module as inputs. Importantly, it is agnostic to the type of input passed to these modules (i.e., it could be text or images). It assumes the _encoder_ and _classifier_ are constructed to handle the data appropriately. Hence, we will see in the next section that we can reuse this model for image inputs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CjV5c5umuY6a"},"outputs":[],"source":["####################################################################################################\n","# DO NOT CHANGE THIS CELL\n","####################################################################################################\n","class SingleModalityClassifierModel(L.LightningModule):\n","    def __init__(self, encoder, classifier, num_classes):\n","        super().__init__()\n","        # model layers\n","        self.encoder = encoder\n","        self.classifier = classifier\n","\n","        # validation metrics\n","        self.val_metrics_tracker = TM.wrappers.MetricTracker(TM.MetricCollection([TM.classification.MulticlassAccuracy(num_classes=num_classes)]))\n","        self.validation_step_outputs = []\n","        self.validation_step_targets = []\n","\n","        # test metrics\n","        self.test_roc = TM.ROC(task=\"multiclass\", num_classes=num_classes, thresholds=list(np.linspace(0.0, 1.0, 20))) # roc and cm have methods we want to call so store them in a variable\n","        self.test_cm = TM.ConfusionMatrix(task='multiclass', num_classes=num_classes)\n","        self.test_metrics_tracker = TM.wrappers.MetricTracker(TM.MetricCollection([TM.classification.MulticlassAccuracy(num_classes=num_classes),\n","                                                            self.test_roc, self.test_cm]))\n","        self.test_step_outputs = []\n","        self.test_step_targets = []\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.classifier(x)\n","        return x\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self.forward(x)\n","        loss = nn.functional.cross_entropy(logits, y)\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self.forward(x)\n","        loss = nn.functional.cross_entropy(logits, y)\n","        self.log('val_loss', loss, on_step=True, on_epoch=True)\n","\n","        # store the outputs and targets for the epoch end step\n","        self.validation_step_outputs.append(logits)\n","        self.validation_step_targets.append(y)\n","        return loss\n","\n","    def on_validation_epoch_end(self):\n","        # stack all the outputs and targets into a single tensor\n","        all_preds = torch.vstack(self.validation_step_outputs)\n","        all_targets = torch.hstack(self.validation_step_targets)\n","\n","        # compute the metrics\n","        loss = nn.functional.cross_entropy(all_preds, all_targets)\n","        self.val_metrics_tracker.increment()\n","        self.val_metrics_tracker.update(all_preds, all_targets)\n","        self.log('val_loss_epoch_end', loss)\n","\n","        # clear the validation step outputs\n","        self.validation_step_outputs.clear()\n","        self.validation_step_targets.clear()\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self.forward(x)\n","        loss = nn.functional.cross_entropy(logits, y)\n","        self.log('test_loss', loss, on_step=True, on_epoch=True)\n","        self.test_step_outputs.append(logits)\n","        self.test_step_targets.append(y)\n","        return loss\n","\n","    def on_test_epoch_end(self):\n","        all_preds = torch.vstack(self.test_step_outputs)\n","        all_targets = torch.hstack(self.test_step_targets)\n","\n","        self.test_metrics_tracker.increment()\n","        self.test_metrics_tracker.update(all_preds, all_targets)\n","        # clear the test step outputs\n","        self.test_step_outputs.clear()\n","        self.test_step_targets.clear()\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n","        return optimizer"]},{"cell_type":"markdown","metadata":{"id":"Mr7C0A2luY6a"},"source":["Now that we've constructed the model, we can train the model using the PyTorch Lightning `Trainer` class. The code cell below implements the training following the same procedure used in Practicum 7."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yhPm5axxuY6a"},"outputs":[],"source":["seed_everything(rs)\n","encoder = TextEncoder(len(dm.vocab), dm.embedding_dim, num_heads=2)\n","classifier = TextClassifier(dm.embedding_dim, 2*dm.max_sample_length(), len(dm.class_name_map))\n","molecular_text_model = SingleModalityClassifierModel(encoder, classifier, num_classes=len(dm.class_name_map))\n","\n","trainer = L.Trainer(default_root_dir=dir_lightning,\n","                    max_epochs=100,\n","                    callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=7)])\n","trainer.fit(model=molecular_text_model, train_dataloaders=dm.train_dataloader(), val_dataloaders=dm.val_dataloader())"]},{"cell_type":"markdown","metadata":{"id":"-EyiIXCQuY6a"},"source":["## Validation Set Performance\n","\n","Below, we can examine performance on the validation set across training epochs. You should see the that validation performance approaches a value near 0.9."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eAZl9wxouY6a"},"outputs":[],"source":["mca = molecular_text_model.val_metrics_tracker.compute_all()['MulticlassAccuracy']\n","plt.plot(range(1, len(mca)+1), mca, marker='o')\n","plt.xlabel('Epoch')\n","plt.ylabel('Epoch Validation Accuracy')\n","plt.grid()"]},{"cell_type":"markdown","metadata":{"id":"vr8vd3SJuY6b"},"source":["## Test Set Performance\n","\n","Now let's examine the model performance on the test set. We follow the same procedure as in Practicum 7 to obtain the model predictions for the test set and view the confusion matrix, ROC curve and classificaiton report. You should see that the text based model performs reasonably well on the test data with an overall f-score near 0.8 and ROC curves that well above the random guessing line. You should also see in the confusion matrix, that the model does misclassify several of the \"uncertain\" examples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dJ8NI7muY6b"},"outputs":[],"source":["trainer.test(model=molecular_text_model, dataloaders=dm.test_dataloader())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQHT31a8uY6b"},"outputs":[],"source":["rslt = molecular_text_model.test_metrics_tracker.compute()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5DI4MmwBuY6b"},"outputs":[],"source":["cmp = sns.heatmap(rslt['MulticlassConfusionMatrix'], annot=True, fmt='d', cmap='Blues')\n","cmp.set_xlabel('Predicted Label')\n","cmp.set_xticklabels(dm.class_name_map.values(), rotation=90)\n","cmp.set_yticklabels(dm.class_name_map.values(), rotation=0)\n","cmp.set_ylabel('Actual Label');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ll2wQyefuY6b"},"outputs":[],"source":["fpr, tpr, thresholds = rslt['MulticlassROC']\n","for i in range(len(dm.class_name_map)):\n","    plt.plot(fpr[i], tpr[i], label=dm.class_name_map[i])\n","plt.xlabel('False Positive Rate')\n","plt.plot([0, 1], [0, 1], 'k--', label='Random')\n","plt.ylabel('True Positive Rate')\n","plt.legend()\n","plt.grid()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ej-PcsPtuY6b"},"outputs":[],"source":["# Print the classification report\n","device = torch.device(\"cpu\")   #\"cuda:0\"\n","molecular_text_model.eval()\n","y_true=[]\n","y_pred=[]\n","with torch.no_grad():\n","    for test_data in dm.test_dataloader():\n","        test_samples, test_labels = test_data[0].to(device), test_data[1].to(device)\n","        pred = molecular_text_model(test_samples).argmax(dim=1)\n","        for i in range(len(pred)):\n","            y_true.append(test_labels[i].item())\n","            y_pred.append(pred[i].item())\n","\n","print(classification_report(y_true,y_pred,target_names=list(dm.class_name_map.values()),digits=4))"]},{"cell_type":"markdown","metadata":{"id":"heZV0G3NuY6b"},"source":["# Image Input Molecular Similarity Classifier\n","\n","Next, let's build a model where the predictions will be made using only the image of the 2D molecular structure. An example image is shown here:\n","\n","![alt text](../source_data/molecular-similarity/images-2D-128x128/image_molecule_000a.png)\n","\n","You can view more examples in the _assignments/source_data/molecular-similarity/images-2D-128x128_ directory. These images have already been resized to all be of the same dimensions (128 x 128 pixels). Additionally, the `MolecularSimilarityDataModule` handles normalizing the pixel values to between [0, 1].  \n","\n","As in the preceeding section, in the following section, you will use the input images to build a classification model that predicts if a given pair of molecules are\n","1. Similar - defined as having a `frac_similar` score between 0.66 and 1.0\n","2. Uncertain - defined as having a `frac_similar` score between 0.33 and 0.66\n","3. Not Similar - defined as having a `frac_similar` score between 0 and 0.33"]},{"cell_type":"markdown","metadata":{"id":"4pC_ydeguY6b"},"source":["# Problem 4 (1 point)\n","\n","In the code cell below, create an instance of `MolecularSimilarityDataModule` that uses the image only modality. The arguments to constructor should be:\n","- data_dir = molecular_data_dir\n","- modalities = 1\n","- batch_size = 10\n","- image_dim = 128\n","- class_threshold_map = ctm\n","- class_name_map = cnm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VVPuBwxRuY6b"},"outputs":[],"source":["seed_everything(rs)\n","ctm = {0:(0, 0.33), 1:(0.33, 0.66), 2:(0.66,1.01)}\n","cnm = {0:'Not Similar', 1:'Uncertain', 2:'Similar'}\n","\n","##### Problem 4 start your code here #####\n","dm = None\n","##### Problem 4 end your code here #####\n","\n","dm.setup()\n","print(len(dm.train_dataloader()))\n","print(len(dm.val_dataloader()))\n","print(len(dm.test_dataloader()))"]},{"cell_type":"markdown","metadata":{"id":"GL4pDmVluY6b"},"source":["Here, we can view a batch of data. Notice that the batch tensor has shape [batch_size, 6, 128, 128]. This is because their are two images to be compared for the molecular simalarity problem, 1 image representing the structure for each model. Here, each image is RGB and thus has 3 channels. The two images are treated as separate channels in the overall input."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mcgpuc4duY6b"},"outputs":[],"source":["imgs, lables = next(iter(dm.train_dataloader()))\n","print(imgs.shape)\n","imgsa = imgs[:, :3, :, :]\n","print(imgsa.shape)\n","grid = make_grid(imgsa, nrow=5)\n","show(grid)\n","imgsb = imgs[:, 3:, :, :]\n","print(imgsb.shape)\n","grid = make_grid(imgsb, nrow=5)\n","show(grid)\n"]},{"cell_type":"markdown","metadata":{"id":"H0rb7zL7uY6b"},"source":["Now that we have the data module ready, we can build the molecular similarity classification model using only the images as inputs. We will design the model following the same approach we used for the text model above and as presented for image data in Practicum 6. Again, the model will include an _encoder_ and a _classifier_. However, this time the _encoder_ will use convolutional layers to create representation of the two image input."]},{"cell_type":"markdown","metadata":{"id":"UWx2m4HiuY6b"},"source":["# Problem 5 (2 points)\n","In the code cell, complete the implementation of the `ImageEncoder` class. In the `__init__` method, you will need to create each of the two `Conv2d` layers that are to be used in the `Sequential` layer that represents the `encoder`.\n","\n","You should use the input arguments to the `__init__` function to set the arguments in the `Conv2d` layers, specifically the first Conv2D layer, `conv2d_1` should use the `input_channels` argument for its `in_channels`, `output_channels[0]` for its `out_channels`, and `kernel_sizes[0]` for its `kernel_size`. Similarly, `conv2d_2` should use the ``output_channels[0]` argument for its `in_channels`, `output_channels[1]` for its `out_channels`, and `kernel_sizes[1]` for its `kernel_size`. For more details, refer to the [PyTorch Conv2D doc](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n","\n","In the forward method, you will need to pass the input, `x` through `encoder` layer.\n","\n","__HINT__: See Practicum 6."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dh8LJfsYuY6b"},"outputs":[],"source":["class ImageEncoder(nn.Module):\n","    def __init__(self, input_channels=3, output_channels=[16,32], kernel_sizes=[5, 3]):\n","        super().__init__()\n","        ##### Problem 5 start your code here #####\n","        conv2d_1 = None\n","        conv2d_2 = None\n","        ##### Problem 5 end your code here #####\n","\n","        self.encoder = nn.Sequential(\n","            conv2d_1\n","            nn.ReLU(),                                             # ReLU activation function\n","            nn.MaxPool2d(kernel_size=2, stride=2),                 # max pooling layer with kernel size 2x2\n","            conv2d_2,\n","            nn.ReLU(),                                             # ReLU activation function\n","            nn.MaxPool2d(kernel_size=2, stride=2),                  # max pooling layer with kernel size 2x2\n","        )\n","\n","    def forward(self, x):\n","        ##### Problem 5 start your code here #####\n","\n","        ##### Problem 5 end your code here #####\n","        return x\n","\n","# Test the ImageEncoder with random input data\n","encoder = ImageEncoder(input_channels=6, output_channels=[16,32])\n","input_image = torch.randn(5, 6, dm.image_dim, dm.image_dim)  # batch_size x channels x height x width\n","output_features = encoder(input_image)\n","print(\"Output shape:\", output_features.shape)"]},{"cell_type":"markdown","metadata":{"id":"fL8pzHv3uY6b"},"source":["# Problem 6 (2 points)\n","In the code cell, complete the implementation of the `ImageClassifier` class. In the `__init__` method, you will need to create the `Flatten` layer and the `linear` feed forward layer. You should use the input arguments to the `__init__` function to set the arguments in the `linear` layer. In the forward method, you will need to pass the input, `x` through both layers.\n","\n","__HINT__: See Practicum 6."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1g0Fj65FuY6b"},"outputs":[],"source":["class ImageClassifier(nn.Module):\n","    def __init__(self, input_dim, num_classes):\n","        super().__init__()\n","        ##### Problem 6 start your code here #####\n","        self.flatten = None\n","        self.linear = None\n","        ##### Problem 6 start your code here #####\n","\n","    def forward(self, x):\n","        ##### Problem 3 start your code here #####\n","\n","        ##### Problem 3 end your code here #####\n","        return x\n","\n","# Test the module with random input data\n","module = ImageClassifier(32*31*31, 3)\n","input_tensor = torch.randn(5, 32, 31, 31)  # Batch size of 5, input tensor shape [5, 32, 7, 7]\n","output = module(input_tensor)\n","print(\"Output shape:\", output.shape)  # Expected output shape: [5, 10]"]},{"cell_type":"markdown","metadata":{"id":"qhjXzAyduY6b"},"source":["We can now build a model that uses only the molecular image data to calculate the similarity class. Importantly, because we can reuse the `MolecularSimilarityDataModule` to construct our model by passing it the new `ImageEncoder` and `ImageClassifier` that will handle the image inputs. Below, we fit the model using the PyTorch Lightning Trainer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9f_37kmTuY6b"},"outputs":[],"source":["seed_everything(rs)\n","encoder = ImageEncoder(input_channels=6, output_channels=[16, 32])\n","classifier = ImageClassifier(32*31*31, len(dm.class_name_map))\n","molecular_image_model = SingleModalityClassifierModel(encoder, classifier, num_classes=len(dm.class_name_map))\n","\n","trainer = L.Trainer(default_root_dir=dir_lightning,\n","                    max_epochs=100,\n","                    callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=7)])\n","trainer.fit(model=molecular_image_model, train_dataloaders=dm.train_dataloader(), val_dataloaders=dm.val_dataloader())"]},{"cell_type":"markdown","metadata":{"id":"tX8d3rUOuY6b"},"source":["## Validation Set Performance\n","\n","Below, we can examine performance on the validation set across training epochs. You should see the that validation performance is not as good as the text based model, approaching a value near 0.5. Why do you think this is?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYM9ar44uY6b"},"outputs":[],"source":["mca = molecular_image_model.val_metrics_tracker.compute_all()['MulticlassAccuracy']\n","plt.plot(range(1, len(mca)+1), mca, marker='o')\n","plt.xlabel('Epoch')\n","plt.ylabel('Epoch Validation Accuracy')\n","plt.grid()"]},{"cell_type":"markdown","metadata":{"id":"D-4EfTX5uY6c"},"source":["Test Set Performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C_0M3Ad6uY6c"},"outputs":[],"source":["trainer.test(model=molecular_image_model, dataloaders=dm.test_dataloader())"]},{"cell_type":"markdown","metadata":{"id":"J1v6zjEauY6c"},"source":["## Test Set Performance\n","\n","Now let's examine the model performance on the test set. We follow the same procedure as for the text based model to obtain the model predictions for the test set and view the confusion matrix, ROC curve and classificaiton report. You should see that the text based model performs poorly compared to the text-based model on the test. This is not surprising given the validation set performance and is likely due to the small dataset size."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sy9VqkaPuY6c"},"outputs":[],"source":["rslt = molecular_image_model.test_metrics_tracker.compute()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"whte41NmuY6c"},"outputs":[],"source":["cmp = sns.heatmap(rslt['MulticlassConfusionMatrix'], annot=True, fmt='d', cmap='Blues')\n","cmp.set_xlabel('Predicted Label')\n","cmp.set_xticklabels(dm.class_name_map.values(), rotation=90)\n","cmp.set_yticklabels(dm.class_name_map.values(), rotation=0)\n","cmp.set_ylabel('Actual Label');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kFhxCwjxuY6c"},"outputs":[],"source":["fpr, tpr, thresholds = rslt['MulticlassROC']\n","for i in range(len(dm.class_name_map)):\n","    plt.plot(fpr[i], tpr[i], label=dm.class_name_map[i])\n","plt.xlabel('False Positive Rate')\n","plt.plot([0, 1], [0, 1], 'k--', label='Random')\n","plt.ylabel('True Positive Rate')\n","plt.legend()\n","plt.grid()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IaLACdmxuY6c"},"outputs":[],"source":["# Print the classification report\n","device = torch.device(\"cpu\")   #\"cuda:0\"\n","molecular_image_model.eval()\n","y_true=[]\n","y_pred=[]\n","with torch.no_grad():\n","    for test_data in dm.test_dataloader():\n","        test_samples, test_labels = test_data[0].to(device), test_data[1].to(device)\n","        pred = molecular_image_model(test_samples).argmax(dim=1)\n","        for i in range(len(pred)):\n","            y_true.append(test_labels[i].item())\n","            y_pred.append(pred[i].item())\n","\n","print(classification_report(y_true,y_pred,target_names=list(dm.class_name_map.values()),digits=4))"]},{"cell_type":"markdown","metadata":{"id":"OI_PJDUjuY6c"},"source":["# Multimodal model (Extra Credit)\n","Now that we've developed a text only and an image only input model, we can ask how we could incorporate both text and image data. There are multiple ways this could be done. Here, we will again use the same conceputal approach where we use an _encoder_ to encode both the image and text data. We will then combine these representations and pass them to a feed forward network that will classify them.\n","\n","We can obtain the text and image data in our batch data by setting `modalities=2` in the `MolecularSimilarityDataModule` constructor. If you examine the code for the `MolecularSimilarityDataModule` class carefully, you will see that in this case the batch data is now a 3-element tuple containing the text, image, and label tensors for the batch. This is because the text and image tensors do not have compatible tensor shapes and thus cannot be easily combined. This will require us to modify our `LightningModule` class to handle this.\n","\n","First, let's createa a data module that provides the multimodal data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-_ToPNBuY6c"},"outputs":[],"source":["seed_everything(rs)\n","class_threshold_map = {0:(0, 0.33), 1:(0.33, 0.66), 2:(0.66,1.01)}\n","class_name_map = {0:'Not Similar', 1:'Uncertain', 2:'Similar'}\n","dm = MolecularSimilarityDataModule(data_dir=os.path.join(dir_dataroot, 'molecular-similarity'), image_dim=128, modalities=2, batch_size=10,\n","                                   class_threshold_map=class_threshold_map, class_name_map=class_name_map)\n","dm.setup()\n","print(len(dm.train_dataloader()))\n","print(len(dm.val_dataloader()))\n","print(len(dm.test_dataloader()))"]},{"cell_type":"markdown","metadata":{"id":"Zu0af9Q9uY6c"},"source":["In our model, we will reuse the `TextEncoder` and `ImageEncoder` classes we created previously. However, we need a new _classifier_ that handles input from these two encoder. This is implemented below. Notice in the `forward` method, that the input argument `x` is now a tuple containing the the output from the text encoder and and the output from the image encoder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vej47NvMuY6c"},"outputs":[],"source":["class ImageTextClassifier(nn.Module):\n","    def __init__(self, input_dim, num_classes):\n","        super().__init__()\n","        self.flatten = nn.Flatten()  # Flatten the input tensor\n","        self.linear = nn.Linear(input_dim, num_classes)  # Linear layer with 10 output units\n","\n","    def forward(self, x):\n","        text_enc = self.flatten(x[0])  # Flatten the input tensor\n","        img_enc = self.flatten(x[1])\n","        # concatenate the text and image encodings\n","        x = torch.hstack((text_enc, img_enc))\n","        x = self.linear(x)   # Pass through the linear layer\n","        return x\n","\n","# Test the module with random input data\n","module = ImageTextClassifier(32*31*31+2*dm.max_sample_length()*dm.embedding_dim, 3)\n","input_image_tensor = torch.randn(5, 32, 31, 31)  # Batch size of 5, input tensor shape [5, 32, 7, 7]\n","input_text_tensor = torch.randn(5, 2*dm.max_sample_length(), dm.embedding_dim)  # Batch size of 5, input tensor shape [5, 256, 64]\n","\n","output = module((input_text_tensor, input_image_tensor))\n","print(\"Output shape:\", output.shape)  # Expected output shape: [5, 10]"]},{"cell_type":"markdown","metadata":{"id":"SatvWjcDuY6c"},"source":["# Problem 7 (2 points extra credit)\n","\n","In the code cell below, complete the `forward` method of the `MultiModalityClassifierModel`. Hint: the input, `x` to the `forward` method is a tuple containing the text input and the image input."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mDmVSqAuY6c"},"outputs":[],"source":["class MultiModalityClassifierModel(L.LightningModule):\n","    def __init__(self, text_encoder, image_encoder, classifier, num_classes):\n","        super().__init__()\n","        # model layers\n","        self.text_encoder = text_encoder\n","        self.image_encoder = image_encoder\n","        self.classifier = classifier\n","\n","        # validation metrics\n","        self.val_metrics_tracker = TM.wrappers.MetricTracker(TM.MetricCollection([TM.classification.MulticlassAccuracy(num_classes=num_classes)]))\n","        self.validation_step_outputs = []\n","        self.validation_step_targets = []\n","\n","        # test metrics\n","        self.test_roc = TM.ROC(task=\"multiclass\", num_classes=num_classes, thresholds=list(np.linspace(0.0, 1.0, 20))) # roc and cm have methods we want to call so store them in a variable\n","        self.test_cm = TM.ConfusionMatrix(task='multiclass', num_classes=num_classes)\n","        self.test_metrics_tracker = TM.wrappers.MetricTracker(TM.MetricCollection([TM.classification.MulticlassAccuracy(num_classes=num_classes),\n","                                                            self.test_roc, self.test_cm]))\n","        self.test_step_outputs = []\n","        self.test_step_targets = []\n","\n","    def forward(self, x):\n","        ##### Problem 7 extra credit start your code here #####\n","\n","        ##### Problem 7 end your code here #####\n","        return x\n","\n","    def training_step(self, batch, batch_idx):\n","        text_batch,image_batch, y = batch\n","        logits = self.forward((text_batch,image_batch))\n","        loss = nn.functional.cross_entropy(logits, y)\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        text_batch, image_batch, y = batch\n","        logits = self.forward((text_batch,image_batch))\n","        loss = nn.functional.cross_entropy(logits, y)\n","        self.log('val_loss', loss, on_step=True, on_epoch=True)\n","\n","        # store the outputs and targets for the epoch end step\n","        self.validation_step_outputs.append(logits)\n","        self.validation_step_targets.append(y)\n","        return loss\n","\n","    def on_validation_epoch_end(self):\n","        # stack all the outputs and targets into a single tensor\n","        all_preds = torch.vstack(self.validation_step_outputs)\n","        all_targets = torch.hstack(self.validation_step_targets)\n","\n","        # compute the metrics\n","        loss = nn.functional.cross_entropy(all_preds, all_targets)\n","        self.val_metrics_tracker.increment()\n","        self.val_metrics_tracker.update(all_preds, all_targets)\n","        self.log('val_loss_epoch_end', loss)\n","\n","        # clear the validation step outputs\n","        self.validation_step_outputs.clear()\n","        self.validation_step_targets.clear()\n","\n","    def test_step(self, batch, batch_idx):\n","        text_batch, image_batch, y = batch\n","        logits = self.forward((text_batch,image_batch))\n","        loss = nn.functional.cross_entropy(logits, y)\n","        self.log('test_loss', loss, on_step=True, on_epoch=True)\n","        self.test_step_outputs.append(logits)\n","        self.test_step_targets.append(y)\n","        return loss\n","\n","    def on_test_epoch_end(self):\n","        all_preds = torch.vstack(self.test_step_outputs)\n","        all_targets = torch.hstack(self.test_step_targets)\n","\n","        self.test_metrics_tracker.increment()\n","        self.test_metrics_tracker.update(all_preds, all_targets)\n","        # clear the test step outputs\n","        self.test_step_outputs.clear()\n","        self.test_step_targets.clear()\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n","        return optimizer"]},{"cell_type":"markdown","metadata":{"id":"DBl3hPxBuY6c"},"source":["The code below will now fit the multimodal model. Note, if you change the number of kernels in the `image_encoder` you will need to update the argument to the `ImageTextClassifier` accordingly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FifxJrR8uY6c"},"outputs":[],"source":["seed_everything(rs)\n","image_encoder = ImageEncoder(input_channels=6, output_channels=[16,32])\n","text_encoder = TextEncoder(len(dm.vocab), dm.embedding_dim, num_heads=2)\n","classifier = ImageTextClassifier(32*31*31+2*dm.max_sample_length()*dm.embedding_dim, num_classes=len(dm.class_name_map))\n","molecular_image_text_model = MultiModalityClassifierModel(text_encoder, image_encoder, classifier, num_classes=len(dm.class_name_map))\n","\n","trainer = L.Trainer(default_root_dir=dir_lightning,\n","                    max_epochs=100,\n","                    callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=7)])\n","trainer.fit(model=molecular_image_text_model, train_dataloaders=dm.train_dataloader(), val_dataloaders=dm.val_dataloader())"]},{"cell_type":"markdown","metadata":{"id":"VLQdK-l1uY6c"},"source":["## Validation Set Performance\n","Here, we again plot the validation set performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MScTIA0uY6c"},"outputs":[],"source":["mca = molecular_image_text_model.val_metrics_tracker.compute_all()['MulticlassAccuracy']\n","plt.plot(range(1, len(mca)+1), mca, marker='o')\n","plt.xlabel('Epoch')\n","plt.ylabel('Epoch Validation Accuracy')\n","plt.grid()"]},{"cell_type":"markdown","metadata":{"id":"8u1zZjJ1uY6d"},"source":["## Test set performance\n","\n","Finally, we can evaluate the test set performance. Overall, we see that the multimodal does not perform as well as the text only model but does perform better than the image only model. Likely, if we had a much larger dataset, we would find that a multimodal would perform best. Of course, this could require substantially more training time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uRqCBop1uY6d"},"outputs":[],"source":["trainer.test(model=molecular_image_text_model, dataloaders=dm.test_dataloader())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KqhfPRELuY6d"},"outputs":[],"source":["rslt = molecular_image_text_model.test_metrics_tracker.compute()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kbX0o8vjuY6d"},"outputs":[],"source":["cmp = sns.heatmap(rslt['MulticlassConfusionMatrix'], annot=True, fmt='d', cmap='Blues')\n","cmp.set_xlabel('Predicted Label')\n","cmp.set_xticklabels(dm.class_name_map.values(), rotation=90)\n","cmp.set_yticklabels(dm.class_name_map.values(), rotation=0)\n","cmp.set_ylabel('Actual Label');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M5IKNp2YuY6d"},"outputs":[],"source":["fpr, tpr, thresholds = rslt['MulticlassROC']\n","for i in range(len(dm.class_name_map)):\n","    plt.plot(fpr[i], tpr[i], label=dm.class_name_map[i])\n","plt.xlabel('False Positive Rate')\n","plt.plot([0, 1], [0, 1], 'k--', label='Random')\n","plt.ylabel('True Positive Rate')\n","plt.legend()\n","plt.grid()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvdCJELvuY6d"},"outputs":[],"source":["# Print the classification report\n","device = torch.device(\"cpu\")   #\"cuda:0\"\n","molecular_image_text_model.eval()\n","y_true=[]\n","y_pred=[]\n","with torch.no_grad():\n","    for test_data in dm.test_dataloader():\n","        test_text, test_img, test_labels = test_data[0].to(device), test_data[1].to(device), test_data[2].to(device)\n","        pred = molecular_image_text_model((test_text, test_img)).argmax(dim=1)\n","        for i in range(len(pred)):\n","            y_true.append(test_labels[i].item())\n","            y_pred.append(pred[i].item())\n","\n","print(classification_report(y_true,y_pred,target_names=list(dm.class_name_map.values()),digits=4))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"dsbase","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":0}
