{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8z_ah_Dvo5l"
      },
      "source": [
        "CPSC 8810 Machine Learning for Biomedical Applications\n",
        "\n",
        "# Assignment 2 - Classifier Model Development with Structured Data\n",
        "# Bacteremia Dataset\n",
        "In this assignment, you are asked to use the `a2-bactermia.csv` dataset to create classification models that predict whether or not bacteria is present in a given blood sample based on input features that include patient age, sex and 50 labratory measurements. The data and the associated data dictionary are contained in the course repository in the _assignments/source_data_ folder. Detailed information can be found in the related journal article [A Risk Prediction Model for Screening Bacteremic Patients: A Cross Sectional Study](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0106765).\n",
        "\n",
        "__Please read through the notebook and follow the instructions for each of the 9 problems.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3VwdRynwAcY",
        "outputId": "2a18f85b-5acf-423e-df62-ca7e9d3956a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Google Colab setup\n",
        "# mount the google drive - this is necessary to access supporting src\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2bZoxI8vo5o"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, learning_curve\n",
        "from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# local project imports\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/CPSC-8810-ML-BioMed/src\")\n",
        "from plotting import plt_box_grid_by_target, plt_box_grid, plt_xy_scatter_grid\n",
        "from cluster_utils import plot_hclust\n",
        "from util import load_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HK2OpALFvo5p"
      },
      "outputs": [],
      "source": [
        "# global settings\n",
        "pd.options.display.max_columns = 100\n",
        "rs = 654321 # random state, use this to ensure reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIkmgS-8vo5p"
      },
      "outputs": [],
      "source": [
        "####################################################################################################\n",
        "# DO NOT CHANGE THIS CELL\n",
        "####################################################################################################\n",
        "# load the bactermia dataset\n",
        "X, y = load_data('bacteremia', '../source_data')\n",
        "#X, X_discard, y, y_discard = train_test_split(X_all, y_all, stratify=y_all, test_size=0.5, random_state=rs)\n",
        "X.head()\n",
        "\n",
        "# load the data dictionary\n",
        "dd = load_data('bacteremia_dictionary', '../source_data')\n",
        "dd.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5S_7CQqvo5q"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuk0__GPvo5q"
      },
      "source": [
        "We will first split the data into a training set (70%) and test set (30%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gf6Q0TANvo5q"
      },
      "outputs": [],
      "source": [
        "# split the data into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=rs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6T56vzA06Db"
      },
      "source": [
        "# READ THIS\n",
        "Before proceeding further, note that the training dataset is rather large. Many of the evaluations in the code cells that follow may take long periods of time to complete on computers with limited resources. It is recommended that you uncomment the code below while you are developing and testing your code. Once you are confident your code is correct, then comment this line out and rerun the entire notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D1O-TYZ08fK"
      },
      "outputs": [],
      "source": [
        "#X_train, y_train = train_test_split(X_train, y_train, stratify=y_train, test_size=0.05, random_state=rs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwNIm_zbvo5q"
      },
      "source": [
        "### Missing Data\n",
        "Next, let's examing the percent missing data for each feature in the training and test set. In a real-world problem, we would need to assess if we suspect any of the features are missing not at random prior to imputation. For the purposes of this assignment, we'll assume all missing observations are either missing completely at random or missing at random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXr4F1eRvo5r"
      },
      "outputs": [],
      "source": [
        "mp_train = (len(X_train.index) - X_train.count())/len(X_train.index)*100\n",
        "mp_test = (len(X_test.index) - X_test.count())/len(X_test.index)*100\n",
        "mp_table = PrettyTable(['Feature', 'Training Missing %', 'Test Missing %'])\n",
        "for j in range(len(mp_train.index)):\n",
        "    label = mp_train.index[j]\n",
        "    if mp_train[label] > 0:\n",
        "        mp_table.add_row([label, f'{mp_train[label]:.1f}', f'{mp_test[label]:.1f}'])\n",
        "print(mp_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7BGF769vo5r"
      },
      "source": [
        "# Problem 1 (1 point)\n",
        "In the code cell below, use _K Nearest Neighbors_ imputation to impute missing observations for training data, `X_train`. Store the the new values in a dataframe variable `X_train_imputed`. Use imputation model fit to the training data to impute the missing observations for the test set, `X_test`. The test data should __not__ be used to fit the imputation model. __HINT__: See practicum 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsq59wrPvo5r"
      },
      "outputs": [],
      "source": [
        "##### Problem 1 Your code here #####\n",
        "imputer = None\n",
        "\n",
        "X_train_imputed = None\n",
        "X_test_imputed = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urV3Z10Kvo5r"
      },
      "source": [
        "# Problem 2 (1 point)\n",
        "In the code cell below, plot kernel density esitmates (KDEs) of the features in training data before and after imputation. Only plot the features with greater than 1% missing observations. The plots should be arranged in a subplot grid with 4 or 5 columns. __HINT__: See practicum 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er3RbSg1vo5r"
      },
      "outputs": [],
      "source": [
        "##### Problem 2 Your code here #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3Vl9wSMvo5r"
      },
      "source": [
        "# Problem 3 (1 point)\n",
        "In the code cell below, standardize the continuuous features (all but `AGE` and `SEX`) of the feature values in the training set, `X_train_imputed` to have zero mean and unit variance. Store the standardized features in a pandas dataframe variable `X_train_scaled`. Using the training data feature means and variances, scale the feature values in the test set, `X_test_imputed` and store the result in a pandas dataframe variable `X_test_scaled`. __HINT__: The scikit-learn `ColumnTransformer` module will be useful here, see Practicum 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON_NsNpivo5r"
      },
      "outputs": [],
      "source": [
        "##### Problem 3 Your code here #####\n",
        "# standardize the continuous and integer features. Do not standardize the binary features.\n",
        "continuous_vars = X.columns.drop(['AGE', 'SEX'])\n",
        "standarizer = None\n",
        "\n",
        "X_train_scaled = None\n",
        "\n",
        "# standardize the continuous and integer features\n",
        "X_test_scaled = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAI82Fkivo5r"
      },
      "source": [
        "# Imbalanced Data\n",
        "In the bar graphs below, we the data balance in the target label for both the training data and the test data. In both cases, only 8% of the samples are positive for bacteremia. It turns out that this imbalance significantly degrades model performance on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1EBWxkivo5r"
      },
      "outputs": [],
      "source": [
        "# count plot of the target variable\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 3))\n",
        "\n",
        "d = len(y_train.index)\n",
        "vc = y_train.value_counts()\n",
        "ax = axes[0]\n",
        "vc.plot(kind='bar', title='Training Data Bacteremia Class Balance', ax=ax)\n",
        "ax.set_xticklabels([f\"No - {vc[0]/d*100:.1f}%\", f\"Yes - {vc[1]/d*100:.1f}%\"], rotation=0)\n",
        "ax.set_xlabel('Bacteremia')\n",
        "ax.set_ylabel('Sample Counts');\n",
        "\n",
        "d = len(y_test.index)\n",
        "vc = y_test.value_counts()\n",
        "ax = axes[1]\n",
        "vc.plot(kind='bar', title='Test Data Baceteremia Class Balance', ax=ax)\n",
        "ax.set_xticklabels([f\"No - {vc[0]/d*100:.1f}%\", f\"Yes - {vc[1]/d*100:.1f}%\"], rotation=0)\n",
        "ax.set_xlabel('Bacteremia')\n",
        "ax.set_ylabel('Sample Counts');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE4Jsakqvo5s"
      },
      "source": [
        "### SMOTE oversampling\n",
        "Because the class imbalance will impact our model performance, we will try applying an over sampling strategy to add postive samples to our data for model training. In the code cell below, the [SMOTE module](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html) from the [imbalanced-lear](https://imbalanced-learn.org/stable/index.html) is used to increase the number of bacteremia postive samples in the _training data only_. The test set is not altered.\n",
        "\n",
        "__OPTIONAL__:<br/>\n",
        "If you are interested to see the impact of the using SMOTE on the model training and results, you can add the following line to the end of the code cell below to nullify the effect of SMOTE oversampling. Then rerun the entire notebook and examine the model performances. <br/><br/>\n",
        "`X_train_resampled, y_train_resampled = X_train_scaled, y_train` <br/><br/>\n",
        "You should see that accuracy and ROC are not significantly impacted, however recall on the test set is reduced to nearly 0% for both models. Please be sure to rerun the notebook with SMOTE enabled before submitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3afhGYTrvo5s"
      },
      "outputs": [],
      "source": [
        "sm = SMOTE(random_state=rs, k_neighbors=10, sampling_strategy='minority')\n",
        "X_train_resampled, y_train_resampled = sm.fit_resample(X_train_scaled, y_train)\n",
        "print(f\"Original training data shape: {X_train_scaled.shape}\")\n",
        "print(f\"Resampled training data shape: {X_train_resampled.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uHk7-v7vo5s"
      },
      "outputs": [],
      "source": [
        "# count plot of the target variable\n",
        "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 3))\n",
        "\n",
        "d = len(y_train_resampled.index)\n",
        "vc = y_train_resampled.value_counts()\n",
        "ax = plt.gca()\n",
        "vc.plot(kind='bar', title='Training Data After Resampling Bacteremia Class Balance', ax=ax)\n",
        "ax.set_xticklabels([f\"No - {vc[0]/d*100:.1f}%\", f\"Yes - {vc[1]/d*100:.1f}%\"], rotation=0)\n",
        "ax.set_xlabel('Bacteremia')\n",
        "ax.set_ylabel('Sample Counts');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOYNSUKhvo5s"
      },
      "source": [
        "# Problem 4 (1 point)\n",
        "In the code cell below, use the scikit-learn `RandomizedSearchCV` module with `StratifiedKFold` cross validation to perform hyperparameter selection for a logitistic regression model using `X_train_resampled` and `y_train_resampled` to fit the model. Your parameter space should include the following tuning parameter values:\n",
        "- `C` in [0.001, 0.01, 0.1, 0.5, 1, 10, 100]\n",
        "- `penalty` in ['l1', 'l2', 'elasticnet', None]\n",
        "- `fit_intercept` in [True, False]\n",
        "\n",
        "Store the best model in the variable `rslt_lr`. The best model should be fit on the entire SMOTE training data, `X_train_resampled` and `y_train_resampled`\n",
        "__HINT:__ See Practicum 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho1_qmQnvo5s"
      },
      "outputs": [],
      "source": [
        "##### Problem 4 Your code here #####\n",
        "model = LogisticRegression(random_state=rs, max_iter=1000, solver='saga', l1_ratio=0.5)\n",
        "\n",
        "# specify the hyperparameter space\n",
        "parameter_space = None\n",
        "\n",
        "# specify the cross-validation method. Use stratified k-fold because the target variable is imbalanced\n",
        "skf = None\n",
        "\n",
        "# specify the budget (number of hyperparameter combinations to try)\n",
        "budget = 20\n",
        "\n",
        "# select a score to optimize\n",
        "score = 'balanced_accuracy'\n",
        "\n",
        "# number of jobs to run in //, -1 means use all CPU processors\n",
        "n_jobs = -1\n",
        "\n",
        "search_lr = None\n",
        "rslt_lr = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS7aR9zxvo5s"
      },
      "outputs": [],
      "source": [
        "print(f'Best {score} score: {rslt_lr.best_score_:.2f}')\n",
        "rslt_lr.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDyTNOL5vo5s"
      },
      "source": [
        "# Problem 5 (1 point)\n",
        "In the code cell below, use the scikit-learn `RandomizedSearchCV` module with `StratifiedKFold` cross validation to perform hyperparameter selection for a random forest model using `X_train_resampled` and `y_train_resampled` to fit the model. Your parameter space should include the following tuning parameter values:\n",
        "- `n_estimators` in [100, 200, 300, 400, 500]\n",
        "- `max_depth` in [4, 8, 10, 12]\n",
        "- `min_samples_leaf` in [1, 2]\n",
        "- `max_features` in ['sqrt', 'log2']\n",
        "Store the best model in the variable `rslt_rf`.\n",
        "\n",
        "The best model should be fit on the entire SMOTE training data, `X_train_resampled` and `y_train_resampled`\n",
        "__HINT:__ See Practicum 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzOS8a-nvo5s"
      },
      "outputs": [],
      "source": [
        "##### Problem 5 Your code here #####\n",
        "# specify the model\n",
        "model = RandomForestClassifier(random_state=rs, bootstrap=True)\n",
        "\n",
        "# specify the hyperparameter space\n",
        "parameter_space = None\n",
        "\n",
        "# specify the cross-validation method. Use stratified k-fold because the target variable is imbalanced\n",
        "skf = None\n",
        "\n",
        "# specify the budget (number of hyperparameter combinations to try)\n",
        "budget = 20\n",
        "\n",
        "# select a score to optimize\n",
        "score = 'balanced_accuracy'\n",
        "\n",
        "# number of jobs to run in //, -1 means use all CPU processors\n",
        "n_jobs = -1\n",
        "\n",
        "search_rf = None\n",
        "rslt_rf = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsBZpp0ovo5s"
      },
      "outputs": [],
      "source": [
        "rslt_rf.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXDhj1auvo5s"
      },
      "outputs": [],
      "source": [
        "print(f'Best {score} score: {rslt_rf.best_score_:.2f}')\n",
        "rslt_rf.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "504T0S7yvo5s"
      },
      "source": [
        "# Model Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YuHktwlvo5s"
      },
      "source": [
        "# Problem 6 (1 point)\n",
        "In the code cell below, assess the logistic regression model performance on the test data `X_test_scaled` and `y_test`. Specifically, provide the following performance assessments:<br/>\n",
        "1. Print the classification report (point metrics)\n",
        "2. Plot the confusion matrix\n",
        "3. Plot the ROC curve\n",
        "4. Plot the Precision-Recall curve\n",
        "__HINT__:See Practicum 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hz6kX79wvo5s"
      },
      "outputs": [],
      "source": [
        "##### Problem 6 Your code here #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZvFGiWLvo5s"
      },
      "source": [
        "# Problem 7 (1 point)\n",
        "In the code cell below, assess the random forest model performance on the test data `X_test_scaled` and `y_test`. Specifically, provide the following performance assessments:<br/>\n",
        "1. Print the classification report (point metrics)\n",
        "2. Plot the confusion matrix\n",
        "3. Plot the ROC curve\n",
        "4. Plot the Precision-Recall curve\n",
        "__HINT__:See Practicum 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b15GMbVRvo5s"
      },
      "outputs": [],
      "source": [
        "##### Problem 7 Your code here #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP3sBOD4vo5s"
      },
      "source": [
        "# Problem 8 (1 point)\n",
        "In the code cell below, us McNemar's test to determine if there is a signficant difference in the predictions made on the test set between the logistic regression model and the random forest model. __HINT__: See Practicum 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nn5_Lcyvo5s"
      },
      "outputs": [],
      "source": [
        "##### Problem 8 Your code here #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iezweUmFvo5s"
      },
      "source": [
        "# Problem 9 (2 points)\n",
        "Assume you are building a system to detect bacteremia for a clinic. The clinicians at the clinic have have informed you that they are very concerned about false positive results. They are willing to accept a minimum sensitivity (recall) of bacteremia cases of 30% in order to minimize false positives. Which model, logistic regression or random forest, would you recommend? Justify your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF-gauzvvo5s"
      },
      "source": [
        "#Problem 9: Enter your response here."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cpce8810",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
