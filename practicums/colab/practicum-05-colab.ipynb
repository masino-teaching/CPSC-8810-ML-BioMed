{"cells":[{"cell_type":"markdown","metadata":{"id":"S87ExoWyvZie"},"source":["# Practicum 05 - Machine Learning Model Development Process\n","\n","In this practicum, we will revisit the [SUPPORT2](https://archive.ics.uci.edu/dataset/880/support2) dataset to create a classification model that predicts in-patient mortality for critically ill patients from data gathered within the first three days of admission. We will follow the full model development process discussed in the last several lectures to:\n","1. Split the data into a training and test set.\n","2. Preprocess the training data to:\n","    1. Impute missing values\n","    2. Standardize it\n","3. Model selection\n","    1. Randomized search for hyperparameter selection\n","    2. Stratified K-fold validation for model selection\n","4. Model Assessment\n","    1. Evaluate performance on the test set\n","    2. Examine learning curves"]},{"cell_type":"code","source":["# Google Colab setup\n","# mount the google drive - this is necessary to access supporting src\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"id":"a24CBZXjve9A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# install any packages not found in the Colab environment\n","!pip install ucimlrepo"],"metadata":{"id":"u0acW4vVvslj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jkEI3Yj0vZie"},"outputs":[],"source":["from ucimlrepo import fetch_ucirepo\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, learning_curve\n","from sklearn.impute import KNNImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n","from prettytable import PrettyTable\n","from functools import reduce\n","from statsmodels.stats.contingency_tables import mcnemar\n","import numpy as np\n","\n","# local project imports\n","import sys\n","sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/CPSC-8810-ML-BioMed/src\")\n","from uci_utils import get_vars_of_type, get_vars_of_type_in_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZayDt4DvZif"},"outputs":[],"source":["# fetch Support2 dataset\n","# fetch Support Dataset\n","# fetch dataset\n","support2 = fetch_ucirepo(id=880)\n","\n","# data (as pandas dataframes)\n","X = support2.data.features.drop(['charges', 'totcst', 'totmcst',\n","                                 'hday', 'dnrday' ], axis=1)\n","y = support2.data.targets['hospdead'] # death in hospital\n","meta_vars = support2.variables\n","feature_type_corrections = [('edu', 'Integer'),\n","                            ('prg6m', 'Continuous'),\n","                            ('adls', 'Categorical'),\n","                            ('diabetes', 'Categorical'),\n","                            ('dementia', 'Categorical')]\n","# several of the features have the wrong type, so we correct them here\n","for tpl in feature_type_corrections:\n","    row = meta_vars[meta_vars.name == tpl[0]].index[0]\n","    meta_vars.loc[row, 'type'] = tpl[1]\n","\n","# convert categorical variables to dummies and create a new dataframe for the features\n","quant_vars, X_quant = get_vars_of_type_in_list(X, meta_vars, var_type_key = 'type', var_name_key = 'name', type_list = ['Continuous', 'Integer', 'Binary'])\n","cat_vars, X_cat = get_vars_of_type(X, meta_vars, var_type_key = 'type', var_name_key = 'name', type_kw = 'Categorical')\n","X_dummy = pd.get_dummies(X_cat, columns=cat_vars,drop_first=True, dtype=int, dummy_na=True)\n","X = pd.concat([pd.DataFrame(X_quant), X_dummy.reset_index(drop=True)], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6FaE4YTvZif"},"outputs":[],"source":["# global settings\n","pd.options.display.max_columns = 100\n","rs = 654321 # random state, use this to ensure reproducibility"]},{"cell_type":"markdown","metadata":{"id":"c0EPvSFavZif"},"source":["# Data Preprocessing\n","Let's begin with data preprocessing in preparaton for model training. Our first step is to split the data into a training set and a held out test set. We set `stratify=y` to ensure the target balance is the same across both sets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gSF-GROSvZif"},"outputs":[],"source":["# split the data into train and test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=rs)"]},{"cell_type":"markdown","metadata":{"id":"LNSONoDXvZig"},"source":["Now let's examine the class balance. We antiticipate that the samples are imbalanced relative to the target label and that we'll wish to address this during model training and selection."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"poFlL6UTvZig"},"outputs":[],"source":["# count plot of the target variable\n","fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n","\n","d = len(y_train.index)\n","vc = y_train.value_counts()\n","ax = axes[0]\n","vc.plot(kind='bar', title='Traing Data Mortality Class Balance', ax=ax)\n","ax.set_xticklabels([f\"No - {vc[0]/d*100:.1f}%\", f\"Yes - {vc[1]/d*100:.1f}%\"], rotation=0)\n","ax.set_xlabel('Patient In Hospital Mortality Outcome')\n","ax.set_ylabel('Sample Counts');\n","\n","d = len(y_test.index)\n","vc = y_test.value_counts()\n","ax = axes[1]\n","vc.plot(kind='bar', title='Test Data Mortality Class Balance', ax=ax)\n","ax.set_xticklabels([f\"No - {vc[0]/d*100:.1f}%\", f\"Yes - {vc[1]/d*100:.1f}%\"], rotation=0)\n","ax.set_xlabel('Patient In Hospital Mortality Outcome')\n","ax.set_ylabel('Sample Counts');"]},{"cell_type":"markdown","metadata":{"id":"ONAZPTnjvZig"},"source":["## Missing Data\n","Next, let's examine the presence of missing data in our feature set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mh5dtVLvZig"},"outputs":[],"source":["mp_train = (len(X_train.index) - X_train.count())/len(X_train.index)*100\n","mp_test = (len(X_test.index) - X_test.count())/len(X_test.index)*100\n","mp_table = PrettyTable(['Feature', 'Training Missing %', 'Test Missing %'])\n","for j in range(len(mp_train.index)):\n","    label = mp_train.index[j]\n","    if mp_train[label] > 0:\n","        mp_table.add_row([label, f'{mp_train[label]:.1f}', f'{mp_test[label]:.1f}'])\n","print(mp_table)"]},{"cell_type":"markdown","metadata":{"id":"lDLp2EPvvZig"},"source":["We can see most features have some fraction of missing observations. We perform imputation to address these. However, note that if we were developing this model for a real-world application, we would need to consider the manner in which the data is missing (i.e., _missing completely at random_, _missing at random_ or _missing not at random_).Features thought to be MNAR would likely need to be dropped from the analysis or possibly treated as indicator variables. For purpsoses of illustration, we will assume the data is either MCAR or MAR."]},{"cell_type":"markdown","metadata":{"id":"gJvDJ4sYvZig"},"source":["### Imputation\n","To address the missing data, we will perform imputation using the [scikit-learn nearest neighbors imputation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer) module. We want to ensure that information from the test data does not _leak_ into the training data. Therefore, we will fit the imputation model using __only__ the training data. We will then use the imputation model fit with the training data to impute values for both the training and test set data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zvr7UkZNvZig"},"outputs":[],"source":["# create an KNNImputer and fit to training data\n","imputer = KNNImputer(n_neighbors=5, weights='uniform')\n","imputer.fit(X_train)\n","X_train_imputed = pd.DataFrame(data=imputer.transform(X_train), columns=X_train.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJP5PmOSvZig"},"outputs":[],"source":["# plot the KDE of the original and imputed data\n","mp_train = (len(X_train.index) - X_train.count())/len(X_train.index)*100\n","keys = [key for key in mp_train.index if mp_train[key] > 0.5]\n","ncols = 4\n","nrows = len(keys) // ncols + 1\n","fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 20))\n","for i, key in enumerate(keys):\n","    ax = axes[i // ncols, i % ncols]\n","    sns.kdeplot(X_train[key], ax=ax, label='original')\n","    sns.kdeplot(X_train_imputed[key], ax=ax, label='imputed')\n","    ax.set_title(key)\n","    ax.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-sotsimwvZig"},"source":["# Problem 1 (1 point)\n","Impute values for missing data on the `X_test`. The imputed values should be assigned to the variable `X_test_imputed`. Be sure to use the `imputer` variable that __was fit to the training data__, `x_train`. The test data should __NOT__ be used to inform the imputation fit. Plot the distributions of the fitted test data as a check on the quality of the imputation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JeCw86G4vZig"},"outputs":[],"source":["## PROBLEM 1 - YOUR CODE HERE ##############################################\n","# Impute the missing values in the test data\n","# Store the imputed data in a new dataframe called X_test_imputed\n","X_test_imputed = None\n","\n","# plot the KDE of the original and imputed data on the test set"]},{"cell_type":"markdown","metadata":{"id":"jmobsbivvZig"},"source":["## Data Standardization\n","\n","Now let's standardize the data to have zero mean and unit variance for the continuous features. As with imputation, we want to prevent test set information from leaking into the training set. Therefore, we will _fit_ our standardization model to the training data and then use it to standardize both the training data and the test data. This amounts to computing a the mean, $\\mu$, and standard deviation, $\\sigma$, on the training data and using those values to center and scale both the training and test sets.\n","\n","We also want to be careful to standardize only the numeric features (continuous and integer). We do __not__ want to standardize the dummy variable (binary) features. We can do this in a single step by using the [scikit-learn column transformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) module. This allows to specify which columns should be altered."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2B7pzvHvZig"},"outputs":[],"source":["# standardize the continuous and integer features on the training data\n","continuous_vars, _ = get_vars_of_type_in_list(X_train_imputed, meta_vars, var_type_key = 'type', var_name_key = 'name', type_list = ['Continuous', 'Integer'])\n","standarizer = ColumnTransformer([(\"standardize\", preprocessing.StandardScaler(), continuous_vars)], remainder='passthrough')\n","standarizer.fit(X_train_imputed)\n","X_train_scaled = pd.DataFrame(standarizer.transform(X_train_imputed), columns=X_train_imputed.columns)"]},{"cell_type":"markdown","metadata":{"id":"1GurKnEOvZih"},"source":["Standardize the test values in `X_test_imputed`. Be sure to use the `standardizer` variable that __was fit to the training data__, `x_train_imputed`. The test data should __NOT__ be used to inform the standardization fit."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lnI4Ib0FvZih"},"outputs":[],"source":["# standardize the continuous and integer features on the test data\n","continuous_vars, _ = get_vars_of_type_in_list(X_test_imputed, meta_vars, var_type_key = 'type', var_name_key = 'name', type_list = ['Continuous', 'Integer'])\n","X_test_scaled = pd.DataFrame(standarizer.transform(X_test_imputed), columns=X_test_imputed.columns)"]},{"cell_type":"markdown","metadata":{"id":"7-fr8B2lvZih"},"source":["# Model Training.\n","Now that we've prepared the data by addressing missing observations and applying standardization, we are ready to train our prediction models. We will consider two models: (1) Logistic regression classifier and (2) Random Forest.\n","\n","As these modesl have servarl tuning parameteres (i.e., hyperparameters that are not learned from the data), we will need to evaluate a variety of hyperparameter combinations in a systematic way to select the optimal values. We will use the scikit-learn [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) module to perform the hyperparameter search.\n","\n","The `RandomizedSearchCV` module will randomly sample hyperparameter value combinations from a user specified dictionary and a budget indicating the number of combinations to test. We will specify a number of number of allowed values for each hyperparameter combination. Importantly, we will include different options of the `class_weight` parameter in both models as a potential mechanism to address class imbalance. Note, our total parameter space will be larger than our budget, so not all hyperparameter combinations will be tested.\n","\n","For each hyperparameter combination, `RandomizedSearchCV` applies cross-validation to score the combination. We will use the scikit-learn [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) module to ensure that the cross-validation folds are balanced relative to the outcome. We will use `balanced_accuracy` as our validation metrics (for other metrics see [Metrics and scoring](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)).\n","\n","## Logistic Regression\n","Let's start by training a logistic regression classifier using the training data with imputed and standardized values, `x_train_scaled`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XU3KdnKXvZih"},"outputs":[],"source":["# specify the model\n","# set the random state for reproducibility\n","# set the soloer to saga to allow for all regularization types\n","# set the l1_ratio to 0.5 to allow for elasticnet mixing when the penalty is elasticnet, ignored otherwise\n","model = LogisticRegression(random_state=rs, max_iter=1000, solver='saga', l1_ratio=0.5)\n","\n","# specify the hyperparameter space\n","parameter_space = {\n","    'C': [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100],\n","    'penalty': ['l1', 'l2', 'elasticnet', None],\n","    'class_weight': [None, 'balanced'],\n","    'fit_intercept': [True, False]\n","}\n","parameter_space_size = reduce(lambda left, right: left*len(right), parameter_space.values(), 1)\n","\n","# specify the cross-validation method. Use stratified k-fold because the target variable is imbalanced\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=rs)\n","\n","# specify the budget (number of hyperparameter combinations to try)\n","budget = 2\n","\n","# select a score to optimize\n","score = 'balanced_accuracy'\n","\n","# number of jobs to run in //, -1 means use all CPU processors\n","n_jobs = -1\n","\n","search_lr = RandomizedSearchCV(estimator=model,\n","                            param_distributions=parameter_space,\n","                            n_iter=budget,\n","                            scoring=score,\n","                            n_jobs=n_jobs,\n","                            cv=skf,\n","                            random_state=rs,\n","                            return_train_score=True,\n","                            verbose=0)\n","rslt_lr = search_lr.fit(X_train_scaled, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLS8iVEevZih"},"outputs":[],"source":["print(f'The search space has {parameter_space_size} hyperparameter combinations.\\nWe evaluated {budget/parameter_space_size*100:.1f}% of them.')"]},{"cell_type":"markdown","metadata":{"id":"kJd_NPc8vZih"},"source":["Let's take a look at the best overall model from the randomized search over hyperparameter combinations. The `rslt_lr` variable includes several attributes that provide information on the search results. These include:\n","1. `best_score_` - the highest cross validation score among the hyperparameter combinations\n","2. `best_params_` - the hyperparameter combination that resulted in the best cross validaton score\n","3. `best_estimator_` - the model with the best hyperparameter combination that has been refit to __all__ of the training data (assuming `refit=True`)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8LZPjKSvZih"},"outputs":[],"source":["print(f'Best {score} score: {rslt_lr.best_score_:.2f}')\n","rslt_lr.best_estimator_"]},{"cell_type":"markdown","metadata":{"id":"QudSqdnwvZih"},"source":["## Random Forest\n","Now let's train a random forest model.\n","\n","# Problem 2 (2 points)\n","In the code cell below, use the same procedure as shown for the logistic regression model to train a random forest model. Specifically, using the defined variables `model`, `parameter_space`, `skf`, `budget`, `score`, and `n_jobs`, create a `RandomizedSearchCV` object and assign it to the `search_rf` variable and fit it to the training data and assign the result to the `reslt_rf` variable.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbjBjQjqvZih"},"outputs":[],"source":["# specify the model\n","# set the random state for reproducibility\n","model = RandomForestClassifier(random_state=rs, bootstrap=True)\n","\n","# specify the hyperparameter space\n","parameter_space = {\n","    'n_estimators': [100, 200, 300, 400, 500],\n","    'max_depth': [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 4, 8],\n","    'max_features': ['sqrt', 'log2'],\n","    'class_weight': [None, 'balanced']\n","}\n","parameter_space_size = reduce(lambda left, right: left*len(right), parameter_space.values(), 1)\n","\n","# specify the cross-validation method. Use stratified k-fold because the target variable is imbalanced\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=rs)\n","\n","# specify the budget (number of hyperparameter combinations to try)\n","# budget = int(0.125*parameter_space_size)\n","budget = 50\n","\n","# select a score to optimize\n","score = 'balanced_accuracy'\n","\n","# number of jobs to run in //, -1 means use all CPU processors\n","n_jobs = -1\n","\n","######################################### PROBLEM 2 - YOUR CODE HERE #####################################################3\n","# create a RandomizedSearchCV using the RandomForestClassifier object and fit it to the training data\n","search_rf = None\n","rslt_rf = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLZOJitavZih"},"outputs":[],"source":["print(f'The search space has {parameter_space_size} hyperparameter combinations.\\nWe evaluated {budget} of them.')"]},{"cell_type":"markdown","metadata":{"id":"yXwQaPt5vZih"},"source":["Let's take a look at the selected random forest hyperparameters and corresponding cross-validation score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0W2zcUDjvZih"},"outputs":[],"source":["print(f'Best {score} score: {rslt_rf.best_score_:.2f}')\n","rslt_rf.best_estimator_"]},{"cell_type":"markdown","metadata":{"id":"04iV4LrXvZih"},"source":["# Model Assessment\n","\n","Now that we've selected our hyperparameters and trained our two models, let's begin assessing their characteristics.\n","\n","## Test set performance\n","Naturally, the first thing we want to consider is the test set performance. Let's start by examining the logistic regression model test set performance. We will examine the confusion matrix, the classifcation report (point metrics), the ROC curve, and the PR curve."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNPIv7RovZih"},"outputs":[],"source":["# set clf to the best logistic regression model\n","clf = rslt_lr.best_estimator_\n","\n","# generate the test predictions\n","y_pred = clf.predict(X_test_scaled)\n","\n","# Plot the ROC\n","fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n","\n","# plot the confusion matrix on the first axis\n","cm = confusion_matrix(y_test, y_pred)\n","disp = ConfusionMatrixDisplay(cm, display_labels=['Morality No', 'Mortality Yes'])\n","disp.plot(cmap='Blues', ax=axes[0])\n","\n","# plot the ROC curve on the second axis\n","disp = RocCurveDisplay.from_estimator(clf, X_test_scaled, y_test, ax=axes[1])\n","axes[1].grid()\n","\n","# Plot the precision-recall curve on the third axis\n","disp = PrecisionRecallDisplay.from_estimator(clf, X_test_scaled, y_test, ax=axes[2])\n","axes[2].grid()\n","\n","print(classification_report(y_test, y_pred, target_names=['Morality No', 'Mortality Yes']))"]},{"cell_type":"markdown","metadata":{"id":"rQ3bATsQvZih"},"source":["# Problem 3 (1 point)\n","As shown above for the logistic regression model, evaluate the Random Forest test set performance by printing the classifcation report, and plotting the confusion matrix, ROC curve, and PR curve."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7cdWUDBvZii"},"outputs":[],"source":["# set clf to the best random forest model\n","clf = rslt_rf.best_estimator_\n","\n","# generate the test predictions\n","y_pred = clf.predict(X_test_scaled)\n","\n","# Plot the ROC\n","fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n","\n","# plot the confusion matrix on the first axis\n","cm = confusion_matrix(y_test, y_pred)\n","disp = ConfusionMatrixDisplay(cm, display_labels=['Morality No', 'Mortality Yes'])\n","disp.plot(cmap='Blues', ax=axes[0])\n","\n","# plot the ROC curve on the second axis\n","disp = RocCurveDisplay.from_estimator(clf, X_test_scaled, y_test, ax=axes[1])\n","axes[1].grid()\n","\n","# Plot the precision-recall curve on the third axis\n","disp = PrecisionRecallDisplay.from_estimator(clf, X_test_scaled, y_test, ax=axes[2])\n","axes[2].grid()\n","\n","print(classification_report(y_test, y_pred, target_names=['Morality No', 'Mortality Yes']))"]},{"cell_type":"markdown","metadata":{"id":"OWvJzaxsvZii"},"source":["## Statistical Significance of Test Set Performance\n","\n","We can see in the classification report and the confusion matrix that the models make somewhat differnt predictions on the test set. Let's evaluate whether the prediction distributions are signficantly different. Since we have only two models that make binary classification predictions, we may apply McNemar's test. To perform McNemar's test, we need to form a contingency table that contains four values:\n","1. `n00`: the number of samples misclassified by both models\n","2. `n01` : the number of samples misclassified by the logistic regression model, but correctly classified by the random forest model\n","3. `n10` : the number of samples correctly classified by the logistic regression model, but misclassified by the random forest model\n","4. `n11` : the number of samples correctly classified by both models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYxaQ28vvZii"},"outputs":[],"source":["significance_threshold = 0.05\n","\n","lr_correct = np.where(rslt_lr.best_estimator_.predict(X_test_scaled)==y_test)\n","rf_correct = np.where(rslt_rf.best_estimator_.predict(X_test_scaled)==y_test)\n","lr_wrong = np.where(rslt_lr.best_estimator_.predict(X_test_scaled)!=y_test)\n","rf_wrong = np.where(rslt_rf.best_estimator_.predict(X_test_scaled)!=y_test)\n","\n","n00 = len(np.intersect1d(lr_wrong, rf_wrong))\n","n01 = len(np.intersect1d(lr_wrong, rf_correct))\n","n10 = len(np.intersect1d(lr_correct, rf_wrong))\n","n11 = len(np.intersect1d(lr_correct, rf_correct))\n","\n","table = [[n00, n01],[n10, n11]]\n","test = mcnemar(table, exact=True)\n","\n","c_table = PrettyTable(['','RF Wrong', 'RF Correct'])\n","c_table.add_row(['LR Wrong', n00, n01])\n","c_table.add_row(['LR Correct', n10, n11])\n","print(c_table)\n","\n","if test.pvalue < significance_threshold:\n","    print(f'The models are significantly different with p-value {test.pvalue:.3f}')"]},{"cell_type":"markdown","metadata":{"id":"uC69SVNavZii"},"source":["## Learning Curve Analyis\n","\n","Now that we see that there is a statistical difference in the models, let's examine the learning curves."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w55VttT3vZii"},"outputs":[],"source":["def plot_learning_curve(model, model_name, scoring='balanced_accuracy', ax=None):\n","    train_sizes = np.linspace(0.1, 1.0, 10)\n","    train_sizes, train_scores, validation_scores = learning_curve(\n","        model, X_train_scaled, y_train, train_sizes=train_sizes, cv=3, scoring=scoring, n_jobs=-1)\n","    train_mean = np.mean(train_scores, axis=1)\n","    train_std = np.std(train_scores, axis=1)\n","\n","    validation_mean = np.mean(validation_scores, axis=1)\n","    validation_std = np.std(validation_scores, axis=1)\n","    if ax is None:\n","        ax = plt.gca()\n","    ax.plot(train_sizes, train_mean, label='Training score', color='blue', marker='o')\n","    ax.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.15)\n","\n","    ax.plot(train_sizes, validation_mean, label='Validation score', color='green', marker='o')\n","    ax.fill_between(train_sizes, validation_mean - validation_std, validation_mean + validation_std, color='green', alpha=0.15)\n","\n","    ax.set_title(f'{model_name} Learning Curve')\n","    ax.set_xlabel('Training Data Size')\n","    ax.set_ylabel(scoring.replace('_', ' ').title())\n","    ax.legend(loc='best')\n","    ax.grid()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CKg6CZUvZii"},"outputs":[],"source":["fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n","plot_learning_curve(rslt_rf.best_estimator_, 'Random Forest', ax=axes[1])\n","plot_learning_curve(rslt_lr.best_estimator_, 'Logistic Regression', ax=axes[0])"]},{"cell_type":"markdown","metadata":{"id":"NcKKkzdbvZin"},"source":["# Problem 4 (2 points)\n","Based on the learning curves:\n","1. Both models display some bias (insuficient capacity). Which model do you think has worse bias and why?\n","2. Which of the two models displays variance (overfitting)? Justify your answer.\n","3. Which model do you think might benefit from additional training samples? Justify you answer."]},{"cell_type":"markdown","metadata":{"id":"NZUF2a2vvZin"},"source":["Problem 4 - your response here."]}],"metadata":{"kernelspec":{"display_name":"cpce8810","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}