{"cells":[{"cell_type":"markdown","metadata":{"id":"9MNlHyYk4T4U"},"source":["# Practicum 07 - Text Classificaiton with Transformer Encoders\n","\n","In this practicum, we will use the [Medical Text Dataset](https://www.kaggle.com/datasets/chaitanyakck/medical-text) (see also [here](https://github.com/sebischair/Medical-Abstracts-TC-Corpus)) to build a classificaiton model that identifies which of five possible disease areas is discussed in an input medical abstract. The models will use only the abstract text as input. We will use [PyTorch](https://pytorch.org/) and [Pytorch Lightning](https://lightning.ai/docs/pytorch/stable/) along with [torchtext](https://pytorch.org/text/stable/index.html) to build the models.\n","\n","We will illulstrate many of the concepts needed to address working with text data including tokenization and token embedding. As in the previous practicums, we will first demonstrate the techniques on a non biomedical dataset. We will then apply those to the _Medical Text Dataset_."]},{"cell_type":"code","source":["# Google Colab setup\n","# mount the google drive - this is necessary to access supporting src\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"id":"1zwirLaz4pZm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# install any packages not found in the Colab environment\n","!pip install lightning\n","!pip install 'portalocker>=2.0.0'"],"metadata":{"id":"0xkW-NdM4qli"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oKl4LnkJ4T4V"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import pytorch_lightning as pl\n","from torchtext.datasets import AG_NEWS\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torch.utils.data import DataLoader\n","from torchtext.datasets import AG_NEWS\n","import torchtext\n","from torchtext.functional import to_tensor\n","import lightning as L\n","from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n","from lightning.pytorch import seed_everything\n","import lightning.pytorch.trainer as trainer\n","import torchmetrics as TM\n","import torchmetrics as TM\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BaGaxQ24T4W"},"outputs":[],"source":["dir_dataroot = \"/content/drive/MyDrive/Colab Notebooks/CPSC-8810-ML-BioMed/data\"\n","\n","dir_lightning = \"/content/drive/MyDrive/Colab Notebooks/CPSC-8810-ML-BioMed/lightning\"\n","\n","rs = 123456 # random seed for everything"]},{"cell_type":"markdown","metadata":{"id":"v6-h0X564T4W"},"source":["# PyTorch Lightning Demonstration on AG_NEWS\n","\n","We will begin by creating a PTL model to classify news articles in the [AG News](https://paperswithcode.com/dataset/ag-news) data set, one of the many datasets available in the [torchtext](https://pytorch.org/text/stable/datasets.html) library. The _AG News_ dataset contains 7,500 training articles and 475 test articles divided into 4 classes. In this example, we will build a deep learing model with an encoder composed of a token embedding layer and 2 transformer layers, followed by a feed forward layer to predict class membership for an input article.\n","\n","First, we need to create a directory for the data. We will create the directory `../../data` which is ignored by git. We also need a direcory to save trained models. By default, PTL will save versions of the model during training called __checkpoints__ as discussed below. Rather than saving these in the current directory, we will a create directory `../../lightning` which is also ignored by git."]},{"cell_type":"markdown","metadata":{"id":"TW__QWER4T4W"},"source":["# Preprocessing Text Data\n","Unlike other data types, text data is somewhat unique in that it is inherently non-numerical. To facilate its use in our machine learning models (or any computational model), we need to convert the input text to a numerical representation. We will do this by:\n","1. Tokening all samples in the dataset (known as a corpus in NLP)\n","2. Forming a fixed length vocabulary composed of the unique tokens with an integer index assigned to each token\n","3. Converting an input text to its token index representation\n","4. Formulating low dimensional numerical vectors (embeddings) for each token\n","\n","Ultimately, we will package all of these steps into a PyTorch Lightning Data Module which will handle these processing steps for use during training and testing. But first, let's look at the steps in detail.\n","\n","Let's start by viewing a sample from the _AG News_ dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7vpKHbci4T4W"},"outputs":[],"source":["print(next(iter(AG_NEWS(split=\"train\"))))"]},{"cell_type":"markdown","metadata":{"id":"y5d_JaPS4T4W"},"source":["Next, we can tokenize the _AG News_ samples using the [torchtext tokenizer utility](https://pytorch.org/text/stable/data_utils.html). We can also create a simple Python generator to combine with the [torchtext build_vocab_from_iterator](https://pytorch.org/text/stable/vocab.html#build-vocab-from-iterator) function to build our _AG News_ vocabulary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5y_agBSZ4T4W"},"outputs":[],"source":["tokenizer = get_tokenizer(\"basic_english\")\n","train_iter = AG_NEWS(split=\"train\")\n","\n","# simple generator to yield list of tokens for all samples\n","def yield_tokens(data_iter):\n","    for _, text in data_iter:\n","        yield tokenizer(text)\n","\n","# build the vocabulary. We add special tokens <unk> and <pad> to handle unknown words for new test samples and padding for model input\n","vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\", \"<pad>\"])\n","vocab.set_default_index(vocab[\"<unk>\"])\n","print(\"Tokens in vocabulary:\", len(vocab))\n","# we see that the vocabulary is a dictionary like object with the token as key and the index as value\n","pad_idx = vocab['<pad>']\n","print(pad_idx)"]},{"cell_type":"markdown","metadata":{"id":"XazfLO1E4T4W"},"source":["Our data module will need to convert input text samples to their respective token index representations during the training, validation, and test steps. Importantly, the samples will be provided in batches. Thus, we will need our training, validation, and test data handlers (which are stored in the data module) to handle this batch conversion from text to token index. This can be done by providing an appropriate function to the data handlers using the `collate_fn` input argument. Recall, that in our ligtning modules we implement `train_step`, `val_step` and `test_step` functions that take a `batch` input that is a tuple containing the input `x` and corresponding label `y` (at least for classificaiton problems). We would like our `collate_fn` to provide the `batch` input in this format.\n","\n","To simplify training, we will require that all inputs are the same length. One way to do this is to process all samples in the dataset and find the longest one (i.e., the one with the most tokens). For any input that is shorter than this one, we will add `<pad>` tokens to make it the same length as the longest sample.\n","\n","To simplify the process, it will useful to have a function that converts a text input to its token index representation. Also, becuase the sample labels are provided as strings that start at 1, we want to convert them to integers starting at 0. We can accomplish both of these tasks with the `lambda` functions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03x-Zcio4T4W"},"outputs":[],"source":["# function to convert text to index representation\n","text_pipeline = lambda x: vocab(tokenizer(x))\n","label_pipeline = lambda x: int(x) - 1"]},{"cell_type":"markdown","metadata":{"id":"dzzkH6Hw4T4X"},"source":["Let's look at the resulting token represenation for the first sample in the _AG News_ dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xf8cIWJi4T4X"},"outputs":[],"source":["\n","label, text = next(iter(AG_NEWS(split=\"train\")))\n","print(text_pipeline(text))\n","label_pipeline(label)"]},{"cell_type":"markdown","metadata":{"id":"1khePl7H4T4X"},"source":["Next, let's find the sample with the most tokens in our input."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tv2FomGb4T4X"},"outputs":[],"source":["max_tokens = 0\n","cnt = 0\n","for label, text in AG_NEWS(split=\"train\"):\n","    l = len(text_pipeline(text))\n","    if l > max_tokens:\n","        max_tokens = l\n","    cnt += 1\n","print(\"Longest training text sample:\", max_tokens)\n","print(\"Number of training samples:\", cnt)"]},{"cell_type":"markdown","metadata":{"id":"NgGqmfwv4T4X"},"source":["## Let's create a reusable Data Module Class\n","Now that we have seen most of the pieces we need to prepare our text intput, let's assemble the data module. We will first need to define custom PyTorch `Dataset` classes to handle loading the samples. Our classes extend the `torch.utils.data.Dataset` class and are required to implement the `__len__()` method which should return the number of samples in the dataset and the `__getitem__()` method which returns the sample at index `idx`. We develop two such classes:\n","1. `TextMapperDataset` - handles datasets provided by [torchtext.datasets](https://pytorch.org/text/stable/datasets.html) without requiring us to use the [torchdata project](https://pytorch.org/data/beta/index.html) which is still in Beta at the time of this writing.\n","2. `TextFileDataset` - handles text data stored in a .csv file where it is assumed that each row is of the form _label, sample_.\n","\n","Finally, we implement the `TextDataModule` class which extends the PyTorch `LightningDataModule` class and can be used with either text file for torchtext datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zIjC72vw4T4X"},"outputs":[],"source":["class TextMapperDataset(torch.utils.data.Dataset):\n","    def __init__(self, data_pipe):\n","        super().__init__()\n","        self.data_pipe = data_pipe\n","        self.samples = self.load_to_memory()\n","        self.length = len(self.samples)\n","\n","    def load_to_memory(self):\n","        samples = []\n","        for _label, _text in self.data_pipe:\n","            samples.append((_text, _label))\n","        return samples\n","\n","    def __len__(self):\n","        return self.length\n","\n","    def __getitem__(self, idx):\n","        return self.samples[idx]\n","\n","class TextFileDataset(TextMapperDataset):\n","    def __init__(self, file_path, skip_header=True):\n","        self.file_path = file_path\n","        self.samples = self.load_to_memory(skip_header)\n","        self.length = len(self.samples)\n","\n","    def load_to_memory(self, skip_header):\n","        samples = []\n","        with open(self.file_path, 'r') as f:\n","            if skip_header:\n","                next(f)\n","            for line in f:\n","                label, text = line.split(',',maxsplit=1)\n","                samples.append((text.replace('\\n', '').replace('\"',''), label))\n","        return samples\n","\n","\n","class TextDataModule(L.LightningDataModule):\n","    def __init__(self, data_source, val_fraction = 0.1,\n","                 tokenizer=\"basic_english\", batch_size=16, embedding_dim=64, max_input_length=512,\n","                 class_name_map=None):\n","        \"\"\"\n","        data_source: str or callable, if str, it is a path to a directory with train.csv and test.csv files\n","        val_fraction: float, fraction of training data to use for validation\n","        tokenizer: str, tokenizer to use\n","        batch_size: int, batch size for training\n","        embedding_dim: int, dimension of word embeddings\n","        max_input_length: int, maximum number of tokens to use in input. The overall maximum input sequence will be set to the\n","        minimum of (truncate longer texts, pad shorter ones)\n","        class_name_map: dict, mapping of class index to class name\n","        \"\"\"\n","        super().__init__()\n","        self.data_source = data_source\n","        self.batch_size = batch_size\n","        self.val_fraction = val_fraction\n","        self.tokenizer = get_tokenizer(tokenizer)\n","        self.embedding_dim = embedding_dim\n","        self.max_tokens = None\n","        self.vocab = None\n","        self.max_input_length = max_input_length\n","        self.class_name_map = class_name_map\n","\n","    def build_vocab(self, data_iterable):\n","        def yield_tokens(data_iter):\n","            for text, _ in data_iter:\n","                yield self.tokenizer(text)\n","        self.vocab = build_vocab_from_iterator(yield_tokens(data_iterable), specials=[\"<unk>\", \"<pad>\"])\n","        self.vocab.set_default_index(self.vocab[\"<unk>\"])\n","        self.padding_index = self.vocab['<pad>']\n","        return self.vocab\n","\n","    def max_tokens_in(self, data_iterable):\n","        if self.vocab is None:\n","            self.build_vocab(data_iterable)\n","        text_to_tokens = lambda x: self.vocab(self.tokenizer(x))\n","        max_tokens = 0\n","        for text, _ in data_iterable:\n","            l = len(text_to_tokens(text))\n","            if l > max_tokens:\n","                max_tokens = l\n","        return max_tokens\n","\n","    def max_sample_length(self):\n","        return min(self.max_input_length, self.max_tokens)\n","\n","    def label_pipeline(self, x):\n","        return int(x) - 1\n","\n","    def text_pipeline(self, x):\n","        return self.vocab(self.tokenizer(x))\n","\n","    def collate_batch(self, batch):\n","        \"\"\"\n","        Collate function to convert a batch of text samples to a tensor of input tokens and a tensor of labels\n","        \"\"\"\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        label_list = []\n","        max_input = min(self.max_tokens, self.max_input_length)\n","        text_batch = torch.zeros((len(batch), max_input), dtype=torch.int64)\n","        pad_idx = self.padding_index\n","        cnt = 0\n","        for _text, _label in batch:\n","            label_list.append(self.label_pipeline(_label))\n","            text = torch.tensor(self.text_pipeline(_text), dtype=torch.int64)\n","            if len(text) > max_input:\n","                text = text[:max_input]\n","            elif len(text) < max_input:\n","                text = torch.concat((text, torch.tensor([pad_idx] * (max_input - len(text)), dtype=torch.int64)))\n","            text_batch[cnt] = text\n","            cnt += 1\n","        label_list = torch.tensor(label_list, dtype=torch.int64)\n","        return text_batch.to(device), label_list.to(device)\n","\n","    def setup(self, stage=None):\n","        if type(self.data_source)==str and os.path.exists(self.data_source): # treat data source as a directory, expect a train and test file\n","            dataset = TextFileDataset(os.path.join(self.data_source, 'train.csv'))\n","            self.max_tokens = self.max_tokens_in(dataset)\n","            test_dataset = TextFileDataset(os.path.join(self.data_source, 'test.csv'))\n","        else:\n","            dataset = TextMapperDataset(self.data_source(split='train'))\n","            self.max_tokens = self.max_tokens_in(dataset)\n","            test_dataset = TextMapperDataset(self.data_source(split='test'))\n","        self.build_vocab(dataset)\n","        n_data = len(dataset)\n","        n_train = int((1-self.val_fraction) * n_data)\n","        n_val = n_data - n_train\n","\n","        train_dataset, val_dataset = torch.utils.data.random_split(dataset, [n_train, n_val])\n","        # self.max_tokens = self.max_tokens_in(dataset)\n","\n","        self._train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_batch)\n","        self._val_dataloader = DataLoader(val_dataset, batch_size=self.batch_size, collate_fn=self.collate_batch)\n","        self._test_dataloader = DataLoader(test_dataset, batch_size=self.batch_size, collate_fn=self.collate_batch)\n","\n","    def train_dataloader(self):\n","        return self._train_dataloader\n","\n","    def test_dataloader(self):\n","        return self._test_dataloader\n","\n","    def val_dataloader(self):\n","        return self._val_dataloader"]},{"cell_type":"markdown","metadata":{"id":"6hHa-9WH4T4X"},"source":["Now that we have a data modoule class, let's create an `TextDataModule` instance for the _AG News_ data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"--F8-RtM4T4X"},"outputs":[],"source":["# load the AG_NEWS dataset\n","seed_everything(rs)\n","dm = TextDataModule(AG_NEWS, batch_size=16, max_input_length=256, class_name_map={0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'})\n","dm.setup()"]},{"cell_type":"markdown","metadata":{"id":"0oam6Qyx4T4X"},"source":["We can examine the dataloaders contained in the `dm` data module to see how many batches are present and verify that it creates the expected token index representations for input samples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSiLUzvz4T4X"},"outputs":[],"source":["print(len(dm.train_dataloader()))\n","print(len(dm.val_dataloader()))\n","print(len(dm.test_dataloader()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-bpeMIO4T4X"},"outputs":[],"source":["x=next(iter(dm.train_dataloader()))\n","print(x[0].shape, x[1].shape)\n","print(x[0][0])\n","print(x[1])"]},{"cell_type":"markdown","metadata":{"id":"mKzaz38K4T4X"},"source":["# Text Classification Model\n","Now we will build a text classificaiton model. Similar to the approach we used for CNN classification of images, we will develop a model with an encoder component and a classifier component. In the encoder, we'll first use an _Embedding_ layer to transform the token index representations of our inputs to a low dimensional vector representation for each token and then use a _Transformer Encoder_ block to further refine the representations using attention mechansisms. The classifier component will pass the Transformer output through a feed forward layer to generate the logits.\n","\n","We will make these components modular by including inputs to the `__init__` constructor to allow us to resuse these classes for models with different parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6czcGorC4T4X"},"outputs":[],"source":["class TextEncoder(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, num_heads,\n","                 num_transformer_layers=2, dim_feedforward=1024, activation='relu', dropout=0.1):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.te_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n","                                              dim_feedforward=dim_feedforward, dropout=dropout,\n","                                              activation=activation, batch_first=True)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=self.te_layer, num_layers=num_transformer_layers)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = self.transformer_encoder(x)\n","        return x\n","\n","# Test the ImageEncoder with random input data\n","encoder = TextEncoder(len(dm.vocab), dm.embedding_dim, num_heads=4)\n","input_text = torch.randint(0, len(dm.vocab), (dm.batch_size, dm.max_sample_length()))\n","print(\"input shape\",input_text.shape)\n","output_features = encoder(input_text)\n","print(\"Output shape:\", output_features.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXUsmbSq4T4Y"},"outputs":[],"source":["class TextClassifier(nn.Module):\n","    def __init__(self, embedding_dim, seq_length, num_classes):\n","        super().__init__()\n","        self.flatten = nn.Flatten()  # Flatten the input tensor\n","        self.linear = nn.Linear(seq_length*embedding_dim , num_classes)  # Linear layer with 10 output units\n","\n","    def forward(self, x):\n","        x = self.flatten(x)  # Flatten the input tensor\n","        x = self.linear(x)   # Pass through the linear layer\n","        return x\n","\n","# Test the module with random input data\n","classifier = TextClassifier(dm.embedding_dim, dm.max_sample_length(), len(dm.class_name_map))\n","input_tensor = torch.randn(dm.batch_size, dm.max_sample_length(), dm.embedding_dim)  # Batch size of 5, input tensor shape [5, 256, 64]\n","print(input_tensor.shape)\n","output = classifier(input_tensor)\n","print(\"Output shape:\", output.shape)  # Expected output shape: [4, 10]"]},{"cell_type":"markdown","metadata":{"id":"1SMckxrY4T4Y"},"source":["Now we will combine our encoder and classifier to form the overall model using the PyTorch LightningModule class as we did in the previous practicum. In fact, although we've changed the class name to `TextClassifierModel`, all of the code is the same as the image classifier we created in the previous lecture. The changes are all contained in the encoder and classifier components."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RydY_bXX4T4Y"},"outputs":[],"source":["class TextClassifierModel(L.LightningModule):\n","    def __init__(self, encoder, classifier, num_classes):\n","        super().__init__()\n","        # model layers\n","        self.encoder = encoder\n","        self.classifier = classifier\n","\n","        # validation metrics\n","        self.val_metrics_tracker = TM.wrappers.MetricTracker(TM.MetricCollection([TM.classification.MulticlassAccuracy(num_classes=num_classes)]))\n","        self.validation_step_outputs = []\n","        self.validation_step_targets = []\n","\n","        # # test metrics\n","        self.test_roc = TM.ROC(task=\"multiclass\", num_classes=num_classes, thresholds=list(np.linspace(0.0, 1.0, 20))) # roc and cm have methods we want to call so store them in a variable\n","        self.test_cm = TM.ConfusionMatrix(task='multiclass', num_classes=num_classes)\n","        self.test_metrics_tracker = TM.wrappers.MetricTracker(TM.MetricCollection([TM.classification.MulticlassAccuracy(num_classes=num_classes),\n","                                                            self.test_roc, self.test_cm]))\n","        self.test_step_outputs = []\n","        self.test_step_targets = []\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.classifier(x)\n","        return x\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self.forward(x)\n","        loss = nn.functional.cross_entropy(logits, y)\n","        self.log('train_loss', loss)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self.forward(x)\n","        loss = nn.functional.cross_entropy(logits, y)\n","        self.log('val_loss', loss, on_step=True, on_epoch=True)\n","\n","        # store the outputs and targets for the epoch end step\n","        self.validation_step_outputs.append(logits)\n","        self.validation_step_targets.append(y)\n","        return loss\n","\n","    def on_validation_epoch_end(self):\n","        # stack all the outputs and targets into a single tensor\n","        all_preds = torch.vstack(self.validation_step_outputs)\n","        all_targets = torch.hstack(self.validation_step_targets)\n","\n","        # compute the metrics\n","        loss = nn.functional.cross_entropy(all_preds, all_targets)\n","        self.val_metrics_tracker.increment()\n","        self.val_metrics_tracker.update(all_preds, all_targets)\n","        self.log('val_loss_epoch_end', loss)\n","\n","        # clear the validation step outputs\n","        self.validation_step_outputs.clear()\n","        self.validation_step_targets.clear()\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self.forward(x)\n","        loss = nn.functional.cross_entropy(logits, y)\n","        self.log('test_loss', loss, on_step=True, on_epoch=True)\n","        self.test_step_outputs.append(logits)\n","        self.test_step_targets.append(y)\n","        return loss\n","\n","    def on_test_epoch_end(self):\n","        all_preds = torch.vstack(self.test_step_outputs)\n","        all_targets = torch.hstack(self.test_step_targets)\n","\n","        self.test_metrics_tracker.increment()\n","        self.test_metrics_tracker.update(all_preds, all_targets)\n","        # clear the test step outputs\n","        self.test_step_outputs.clear()\n","        self.test_step_targets.clear()\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n","        return optimizer"]},{"cell_type":"markdown","metadata":{"id":"6sMtnueq4T4Y"},"source":["## Model Training\n","Now we are ready to train and evaluate our model. We follow the same procedure as the previous practicum, using the PyTorch Lightning `Trainer` class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oA1IZceX4T4Y"},"outputs":[],"source":["seed_everything(rs)\n","encoder = TextEncoder(len(dm.vocab), dm.embedding_dim, num_heads=4)\n","classifier = TextClassifier(dm.embedding_dim, dm.max_sample_length(), len(dm.class_name_map))\n","agnews_model = TextClassifierModel(encoder, classifier, num_classes=len(dm.class_name_map))\n","\n","trainer = L.Trainer(default_root_dir=dir_lightning,\n","                    max_epochs=5,\n","                    callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\n","trainer.fit(model=agnews_model, train_dataloaders=dm.train_dataloader(), val_dataloaders=dm.val_dataloader())"]},{"cell_type":"markdown","metadata":{"id":"STDRw4fK4T4Y"},"source":["## Validation Set Accuracy\n","Next, let's examine the validation set accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PG-AIGDV4T4Y"},"outputs":[],"source":["mca = agnews_model.val_metrics_tracker.compute_all()['MulticlassAccuracy']\n","plt.plot(range(1, len(mca)+1), mca, marker='o')\n","plt.xlabel('Epoch')\n","plt.ylabel('Epoch Validation Accuracy')\n","plt.grid()"]},{"cell_type":"markdown","metadata":{"id":"do040mkk4T4Y"},"source":["# Test Set Performance\n","Now that we've trained the model, we're ready to examine the test set accuracy. Here, we use the PyTorch Lightning Trainer module's _test_ function passing in the test dataloader from our datamodule."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_4if4ht4T4Y"},"outputs":[],"source":["trainer.test(model=agnews_model, dataloaders=dm.test_dataloader())"]},{"cell_type":"markdown","metadata":{"id":"VQj1ttin4T4Y"},"source":["Next, we gather the performance metrics from the model's `test_metrics_tracker` attribute using the `compute` method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llQZn00_4T4Y"},"outputs":[],"source":["rslt = agnews_model.test_metrics_tracker.compute()"]},{"cell_type":"markdown","metadata":{"id":"gMQXUMsu4T4Y"},"source":["Now, we can plot the confusion matrix and the class level ROC curves to assess performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxeJ2d614T4Y"},"outputs":[],"source":["cmp = sns.heatmap(rslt['MulticlassConfusionMatrix'], annot=True, fmt='d', cmap='Blues')\n","cmp.set_xlabel('Predicted Label')\n","cmp.set_xticklabels(dm.class_name_map.values(), rotation=90)\n","cmp.set_yticklabels(dm.class_name_map.values(), rotation=0)\n","cmp.set_ylabel('Actual Label');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8T2AjMqi4T4Y"},"outputs":[],"source":["fpr, tpr, thresholds = rslt['MulticlassROC']\n","for i in range(len(dm.class_name_map)):\n","    plt.plot(fpr[i], tpr[i], label=dm.class_name_map[i])\n","plt.xlabel('False Positive Rate')\n","plt.plot([0, 1], [0, 1], 'k--', label='Random')\n","plt.ylabel('True Positive Rate')\n","plt.legend()\n","plt.grid()"]},{"cell_type":"markdown","metadata":{"id":"wVzCm4pF4T4Y"},"source":["Finally, let's get the classification report."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"apAH_1W24T4Y"},"outputs":[],"source":["# Print the classification report\n","device = torch.device(\"cpu\")   #\"cuda:0\"\n","agnews_model.eval()\n","y_true=[]\n","y_pred=[]\n","with torch.no_grad():\n","    for test_data in dm.test_dataloader():\n","        test_samples, test_labels = test_data[0].to(device), test_data[1].to(device)\n","        pred = agnews_model(test_samples).argmax(dim=1)\n","        for i in range(len(pred)):\n","            y_true.append(test_labels[i].item())\n","            y_pred.append(pred[i].item())\n","\n","print(classification_report(y_true,y_pred,target_names=list(dm.class_name_map.values()),digits=4))"]},{"cell_type":"markdown","metadata":{"id":"QcH-iU6u4T4e"},"source":["# Medical Abstract Classificaiton\n","Now let's tryout our model on the [medical abstract dataset](https://www.kaggle.com/datasets/chaitanyakck/medical-text/data). This dataset is much smaller (about 1/10 the size) of the _AG News_ dataset, so we should not expect to see the same level of performance."]},{"cell_type":"markdown","metadata":{"id":"9uOCh3H74T4e"},"source":["# Problem 1 (2 points)\n","In the code cell below, use the `TextDataModule` class to create a data module for the medical abstract dataset using the train.csv and test.csv files in the _../../data/medical-abstracts-tc-corpus_ directory. In your inputs to the `TextDataModule` constructor, set\n","````\n","batch_size=16\n","max_input_length=512\n","embedding_dim=64\n","````"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"shjbHr6X4T4e"},"outputs":[],"source":["seed_everything(rs)\n","medical_dir = os.path.join(dir_dataroot,\"medical-abstracts-tc-corpus\")\n","class_map = {0: 'Neoplasms', 1: 'Digestive System', 2: 'Nervous System', 3: 'Cardiovasicular', 4: 'Generic'}\n","\n","########## YOUR CODE HERE ############\n","dm = None\n","\n","########## YOUR CODE HERE ############"]},{"cell_type":"markdown","metadata":{"id":"ydE15OSR4T4e"},"source":["Let's see how many batches are in the training, validation, and test datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hiu2hGMd4T4e"},"outputs":[],"source":["print(len(dm.train_dataloader()))\n","print(len(dm.val_dataloader()))\n","print(len(dm.test_dataloader()))"]},{"cell_type":"markdown","metadata":{"id":"renrzirf4T4e"},"source":["# Problem 2 (2 points)\n","In the code cell below, create a TextEncoder, TextClassifier, and TextClassifierModel and then train the model. In the encoder, set `num_heads=2`. The other model attributes should be set using the DataModule variable, `dm` in the same way they were for the _AG News_ model above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mVuDhUWy4T4e"},"outputs":[],"source":["seed_everything(rs)\n","########## START YOUR CODE HERE ############\n","encoder = None\n","classifier = None\n","medical_model = None\n","########## END YOUR CODE HERE ############\n","\n","trainer = L.Trainer(default_root_dir=dir_lightning,\n","                    max_epochs=50,\n","                    callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)])\n","trainer.fit(model=medical_model, train_dataloaders=dm.train_dataloader(), val_dataloaders=dm.val_dataloader())"]},{"cell_type":"markdown","metadata":{"id":"u5vf9ROs4T4e"},"source":["## Validation Set Accuracy\n","Now let's examine the validation set accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tqWnsrli4T4e"},"outputs":[],"source":["mca = medical_model.val_metrics_tracker.compute_all()['MulticlassAccuracy']\n","plt.plot(range(1, len(mca)+1), mca, marker='o')\n","plt.xlabel('Epoch')\n","plt.ylabel('Epoch Validation Accuracy')\n","plt.grid()"]},{"cell_type":"markdown","metadata":{"id":"u8QXeWY74T4e"},"source":["## Test Set\n","Finally, let's examine the test set accuracy including the confusion matrix, ROC curves, and print the classification report."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"loN1Z8vm4T4e"},"outputs":[],"source":["trainer.test(model=medical_model, dataloaders=dm.test_dataloader())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FN0PKDjn4T4e"},"outputs":[],"source":["rslt = medical_model.test_metrics_tracker.compute()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cotiTzYx4T4e"},"outputs":[],"source":["cmp = sns.heatmap(rslt['MulticlassConfusionMatrix'], annot=True, fmt='d', cmap='Blues')\n","cmp.set_xlabel('Predicted Label')\n","cmp.set_xticklabels(dm.class_name_map.values(), rotation=90)\n","cmp.set_yticklabels(dm.class_name_map.values(), rotation=0)\n","cmp.set_ylabel('Actual Label');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OLOR_zlc4T4e"},"outputs":[],"source":["fpr, tpr, thresholds = rslt['MulticlassROC']\n","for i in range(len(dm.class_name_map)):\n","    plt.plot(fpr[i], tpr[i], label=dm.class_name_map[i])\n","plt.xlabel('False Positive Rate')\n","plt.plot([0, 1], [0, 1], 'k--', label='Random')\n","plt.ylabel('True Positive Rate')\n","plt.legend()\n","plt.grid()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Znm8rqGS4T4f"},"outputs":[],"source":["# Print the classification report\n","device = torch.device(\"cpu\")   #\"cuda:0\"\n","medical_model.eval()\n","y_true=[]\n","y_pred=[]\n","with torch.no_grad():\n","    for test_data in dm.test_dataloader():\n","        test_samples, test_labels = test_data[0].to(device), test_data[1].to(device)\n","        pred = medical_model(test_samples).argmax(dim=1)\n","        for i in range(len(pred)):\n","            y_true.append(test_labels[i].item())\n","            y_pred.append(pred[i].item())\n","\n","print(classification_report(y_true,y_pred,target_names=list(dm.class_name_map.values()),digits=4))"]},{"cell_type":"markdown","metadata":{"id":"qSX7wl6_4T4f"},"source":["# Problem 3 (2 points)\n","In the markdown cell below, provide your interpretation of the medical abstract classification model performance. Specifically, why do you think the model performance is poor compared to the _AG News_ model. What do you notice in the confusion matrix and the ROC curves that support your interpretation?"]},{"cell_type":"markdown","metadata":{"id":"RI5UPUXW4T4f"},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}