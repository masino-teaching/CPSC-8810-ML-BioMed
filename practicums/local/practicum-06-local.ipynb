{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicum 06 - Image Classificaiton with CNNs\n",
    "\n",
    "In this practicum, we will use the [Alzheimer MRI Preprocessed Dataset](https://www.kaggle.com/datasets/sachinkumar413/alzheimer-mri-dataset) to build a classificaiton model that estimates dimentia severity for individuals with Alzheimer's disease. The models will use only the MRI image as input. We will use [PyTorch](https://pytorch.org/) and [Pytorch Lightning](https://lightning.ai/docs/pytorch/stable/) to build the models. \n",
    "\n",
    "We will illulstrate many of the primary concepts of both PyTorch (PT) and PyTorch Lightning (PTL) (though certainly not all) that are needed to build deep learning models including\n",
    "1. PyTorch\n",
    "    1. Tensors\n",
    "    2. nn.Modules\n",
    "2. PyTorch Lightning\n",
    "    1. Data modules and Data loaders\n",
    "    2. Ligtning modules\n",
    "    3. Callbacks\n",
    "    4. Early stopping\n",
    "    5. Metrics\n",
    "We will also use the [TorchMetrics](https://lightning.ai/docs/torchmetrics/stable/) library to simplify performance metric calcuation.\n",
    "\n",
    "For more information on PyTorch and PyTorch Lightning, see these tutorials:\n",
    "1. [PyTorch Tutorials](https://pytorch.org/tutorials/)\n",
    "2. [PyTorch Lightning Tutorials](https://lightning.ai/docs/pytorch/stable/levels/core_skills.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision.transforms.functional as F\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch import seed_everything\n",
    "import torchmetrics as TM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../src') # alternatively add to path using: pip install -e /path/to/src\n",
    "from torchvision_utils import show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Lightning Demonstration on FashionMNIST\n",
    "\n",
    "We will begin by creating a PTL model to classify images from the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist?tab=readme-ov-file) data set, one of the many datasets available in the [PyTorch torchvision](https://pytorch.org/vision/stable/datasets.html) library. The FashionMNIST dataset contains 60,000 training images and 10,000 test images of clothing divided into 10 classes. In this example, we will build a deep learing model with 2 convolutional layers followed by a feed forward layer to predict class membership for an input image.\n",
    "\n",
    "First, we need to create a directory for the FashionMNIST data. We will create the directory `../../data` which is ignored by git. We also need a direcory to save trained models. By default, PTL will save versions of the model during training called __checkpoints__ as discussed below. Rather than saving these in the current directory, we will a create directory `../../lightning` which is also ignored by git.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_directory(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "dir_dataroot = os.path.join(\"..\", \"..\", \"data\")\n",
    "create_data_directory(dir_dataroot)\n",
    "\n",
    "dir_lightning = os.path.join(\"..\", \"..\", \"lightning\")\n",
    "create_data_directory(dir_lightning)\n",
    "\n",
    "rs = 123456 # random seed for everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets, data loaders, data modules\n",
    "\n",
    "Now we can pull the data using the `torchvision.datasets.FashionMNIST` which is an instance of PT [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset). When we first instantiate the `FashionMNIST` object, it will automatically download the data to the specificed directory. The download will include training and test data, however our object will only reference the training data. We will split this data into a validation and training set and use those to create PT [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)s. We will then create a second instance of the FashionMNIST dataset setting the argument `train=False` to obtain the test data. We will create a third `DataLoader` with the test data. We will use these data loaders to train and test the model.\n",
    "\n",
    "### Why all these data handlers?\n",
    "Unlike the models we've seen previously that used tabular data that we loaded entirely into memory, most deep learning models (such as the ones developed here) use unstructured data that is stored in many files and usually is too large to load into memory at once. Similarly, the models cannot be trained with all of the data in a given iteration. Instead we select small subsets of the data, called __batches__, that we use to update the model during learning (so-called __batch training__). Model training is divided into __epochs__. Assuming there are $n$ samples in the training data and the batch size has $k<<n$ samples, the model will updated $n/k$ (or $n/k +1$) times during each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed for everything\n",
    "seed_everything(rs)\n",
    "batch_size = 16\n",
    "ds_fashion_train_all = FashionMNIST(dir_dataroot, train=True, download=True, transform=ToTensor())\n",
    "\n",
    "# split into train and validation\n",
    "train_size = int(0.8 * len(ds_fashion_train_all))\n",
    "val_size = len(ds_fashion_train_all) - train_size\n",
    "ds_fashion_train, ds_fashion_val = utils.data.random_split(ds_fashion_train_all, [train_size, val_size])\n",
    "train_loader = DataLoader(ds_fashion_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(ds_fashion_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# get test data\n",
    "ds_fashion_test = FashionMNIST(dir_dataroot, train=False, download=True, transform=ToTensor())\n",
    "test_loader = DataLoader(ds_fashion_test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Tensors\n",
    "[PyTorch Tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html) are the primary data structures used in PyTorch models. PT Tensors have many of the same properties as numpy ndarrays. In fact a PT Tensor, `x` can be converted to a numpy array by `na = x.numpy()` and a PT Tensor can be created from an numpy array, `na`, with `x = torch.from_numpy(na)`.\n",
    "\n",
    "Here we examine the shape of the FashionMNIST tensors. Here, the first dimesnion indicates the number of samples, and the second and third dimensions are the size of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training samples:\",ds_fashion_train_all.data[ds_fashion_train.indices].shape)\n",
    "print(\"Unique classes in training:\",ds_fashion_train_all.targets[ds_fashion_train.indices].unique())\n",
    "\n",
    "print(\"\\nValidation samples:\", ds_fashion_train_all.data[ds_fashion_val.indices].shape)\n",
    "print(\"Unique classes in validation:\", ds_fashion_train_all.targets[ds_fashion_val.indices].unique())\n",
    "\n",
    "print(\"\\nTest samples:\", ds_fashion_test.data.shape)\n",
    "print(ds_fashion_test.targets.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access each sample as a 2D tensor by either indexing on the dataset, `ds_fashion_train`, or wrapping the train_loader in an `iter`. Here we display a batch of images from the training data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs = [ds_fashion_train[i][0] for i in range(batch_size)]\n",
    "imgs = next(iter(train_loader))[0]\n",
    "print(imgs.shape)\n",
    "grid = make_grid(imgs)\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules & Models\n",
    "Let's start building our FashionMNIST classification model. Conceptually, we will think of this model as a pipeline with two modules:\n",
    "1. Enocder - this portion of the model will contain our convolutional layers and will output a _representation_ of the input image in the form of many small tensors resulting from the convolution process.\n",
    "2. Classifier - this portion will contain a fully connected layer that will transorm the encoder representation to class label `logits`. Note, we will not need to explicitly include a softmax function as this can be handled external to the model.\n",
    "\n",
    "We can create our modules by extending the [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) class or one of its [many subclases](https://pytorch.org/docs/stable/nn.html). Most often we will only need to concer ourselves with the `__init__` method, where we will specify the subparts of the module and the `forward` method where we will specify how a given input should be processed.\n",
    "\n",
    "Below, we create an `ImageEncoder` module that contains 2 convolutional layers each followed by a `ReLU` activation function and a max pooling operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # input has 1 channel\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # conv2d layer with 1 input channel, 16 filters, kernel size 3x3\n",
    "            nn.ReLU(),                                             # ReLU activation function\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # max pooling layer with kernel size 2x2\n",
    "            # input has 16 channels\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1), # conv2d layer with 16 input channels, 32 filters, kernel size 3x3\n",
    "            nn.ReLU(),                                             # ReLU activation function\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)                  # max pooling layer with kernel size 2x2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "# Test the ImageEncoder with random input data\n",
    "encoder = ImageEncoder()\n",
    "input_image = torch.randn(1, 1, 28, 28)  # batch_size x channels x height x width\n",
    "output_features = encoder(input_image)\n",
    "print(\"Output shape:\", output_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create our classifier module that contains a fully connected feed forward network. It also used a `Flatten` module to combine the incoming input (which will be the output tensors of the convolutional encoder) into a single 1D tensor that is passed to the feed forward layer in the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()  # Flatten the input tensor\n",
    "        self.linear = nn.Linear(32 * 7 * 7, 10)  # Linear layer with 10 output units\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # Flatten the input tensor\n",
    "        x = self.linear(x)   # Pass through the linear layer\n",
    "        return x\n",
    "\n",
    "# Test the module with random input data\n",
    "classifier = Classifier()\n",
    "input_tensor = torch.randn(5, 32, 7, 7)  # Batch size of 5, input tensor shape [5, 32, 7, 7]\n",
    "output = classifier(input_tensor)\n",
    "print(\"Output shape:\", output.shape)  # Expected output shape: [5, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our modules, we can compose them into the final model. Here, we use the PTL [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) class to build the model. The PTL Lightning module is a an extension of the PT nn.Module class that, when used with the PTL [Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html) will greatly reduce the amount of code needed to managing the model training process (more below.) In the PTL LigthningModlue class, there are several methods we usually will want to overload:\n",
    "1. `__init__` : similar to nn.Module - define the model components and setup behavior\n",
    "2. `forward` : same as nn.Module\n",
    "3. `training_step` : define the loss computation for a single training batch \n",
    "4. `validation_step` : define the loss computation for a single validation batch (by default, this is done at the end of each epoch). Can be used for things like early stopping.\n",
    "5. `test_step` : define the loss computation for a single test batch \n",
    "6. `configure_optimizers` : used to specify the optimization object that will update the model parameters given a loss value\n",
    "\n",
    "We may also want to use the PTL LightningModule __call back hooks__ such as `on_validation_epoch_end` and `on_test_epoch_end` that allow us to define actions that occur between epochs.\n",
    "\n",
    "We will also make use of the [TorchMetrics](https://lightning.ai/docs/torchmetrics/stable/) library\n",
    "\n",
    "Below we build an ImageClassifier model that takes as input an encoder and a classifier (both of which are expected to be nn.Modules) and a scalar, `num_classes`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifierModel(L.LightningModule):\n",
    "    def __init__(self, encoder, classifier, num_classes):\n",
    "        super().__init__()\n",
    "        # model layers\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        \n",
    "        # validation metrics\n",
    "        self.val_metrics_tracker = TM.wrappers.MetricTracker(TM.MetricCollection([TM.classification.MulticlassAccuracy(num_classes=num_classes)]))\n",
    "        self.validation_step_outputs = []\n",
    "        self.validation_step_targets = []\n",
    "\n",
    "        # test metrics\n",
    "        self.test_roc = TM.ROC(task=\"multiclass\", num_classes=num_classes, thresholds=list(np.linspace(0.0, 1.0, 20))) # roc and cm have methods we want to call so store them in a variable\n",
    "        self.test_cm = TM.ConfusionMatrix(task='multiclass', num_classes=num_classes)\n",
    "        self.test_metrics_tracker = TM.wrappers.MetricTracker(TM.MetricCollection([TM.classification.MulticlassAccuracy(num_classes=num_classes), \n",
    "                                                            self.test_roc, self.test_cm]))\n",
    "        self.test_step_outputs = []\n",
    "        self.test_step_targets = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True)\n",
    "        \n",
    "        # store the outputs and targets for the epoch end step\n",
    "        self.validation_step_outputs.append(logits)\n",
    "        self.validation_step_targets.append(y)\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        # stack all the outputs and targets into a single tensor\n",
    "        all_preds = torch.vstack(self.validation_step_outputs)\n",
    "        all_targets = torch.hstack(self.validation_step_targets)\n",
    "        \n",
    "        # compute the metrics\n",
    "        loss = nn.functional.cross_entropy(all_preds, all_targets)\n",
    "        self.val_metrics_tracker.increment()\n",
    "        self.val_metrics_tracker.update(all_preds, all_targets)\n",
    "        self.log('val_loss_epoch_end', loss)\n",
    "        \n",
    "        # clear the validation step outputs\n",
    "        self.validation_step_outputs.clear()\n",
    "        self.validation_step_targets.clear()\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        self.log('test_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.test_step_outputs.append(logits)\n",
    "        self.test_step_targets.append(y)\n",
    "        return loss\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        all_preds = torch.vstack(self.test_step_outputs)\n",
    "        all_targets = torch.hstack(self.test_step_targets)\n",
    "        \n",
    "        self.test_metrics_tracker.increment()\n",
    "        self.test_metrics_tracker.update(all_preds, all_targets)\n",
    "        # clear the test step outputs\n",
    "        self.test_step_outputs.clear()\n",
    "        self.test_step_targets.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Now that we have specified the model architecture we can train the model. Here we will use the PTL [Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html) class. The `Trainer` will handle all of the training process: sampling batches from the training data, calling the model train step, calling the optimizer, updating the model parameters, storing training metrics, etc. \n",
    "\n",
    "Importantly, the `trainer` will also automatically check if a __GPU__ is available and if so, handle transferring the data to and from the GPU as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(rs)\n",
    "encoder = ImageEncoder()\n",
    "classifier = Classifier()\n",
    "fashion_model = ImageClassifierModel(encoder, classifier, num_classes=10)\n",
    "\n",
    "trainer = L.Trainer(default_root_dir=dir_lightning, \n",
    "                    max_epochs=5,\n",
    "                    callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)])\n",
    "trainer.fit(model=fashion_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Accuracy\n",
    "The validation step performance metric calculations are stored in the variable `fashion_model.val_metrics_tracker`. Calling the `compute_all` method on this variable returns a dictionary like object containing the any metrics, in this case __multi class accuracy_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca = fashion_model.val_metrics_tracker.compute_all()['MulticlassAccuracy']\n",
    "plt.plot(range(1, len(mca)+1), mca, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Epoch Validation Accuracy')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Performance\n",
    "Now let's take a look at the test set performance. We can again use the `trainer` to run the model on the test data by calling the `trainer.test` method and passing it the `test_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=fashion_model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our test step performance metric calculations are stored in the variable `fashion_model.test_metrics_tracker`. Calling the `compute` method on this variable returns a dictionary like object containing the any metrics, in this case _MulticlassConfusionMatrix__ and __MulticlassROC__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = fashion_model.test_metrics_tracker.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "cmp = sns.heatmap(rslt['MulticlassConfusionMatrix'], annot=True, fmt='d', cmap='Blues')\n",
    "cmp.set_xlabel('Predicted Label')\n",
    "cmp.set_xticklabels(ds_fashion_train_all.classes, rotation=90)\n",
    "cmp.set_yticklabels(ds_fashion_train_all.classes, rotation=0)\n",
    "cmp.set_ylabel('Actual Label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 1 vs all ROC curves\n",
    "fpr, tpr, thresholds = rslt['MulticlassROC']\n",
    "for i in range(10):\n",
    "    plt.plot(fpr[i], tpr[i], label=ds_fashion_train_all.classes[i])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report\n",
    "device = torch.device(\"cpu\")   #\"cuda:0\"\n",
    "fashion_model.eval()\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_images, test_labels = test_data[0].to(device), test_data[1].to(device)\n",
    "        pred = fashion_model(test_images).argmax(dim=1)\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())\n",
    "\n",
    "print(classification_report(y_true,y_pred,target_names=ds_fashion_train_all.classes,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading saved models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that during training, PTL saved `.ckpt` files to the `../../lightning` directory. These are trained versions of the model that can be loaded directly into a new instance of the model. In turn, these can be used for further training or inference on new test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from checkpoint\n",
    "# YOU MAY NEED TO UPDATE THE PATH TO THE CHECKPOINT FILE\n",
    "enc = ImageEncoder()\n",
    "clf = Classifier()\n",
    "mp = os.path.join(dir_lightning, 'lightning_logs/version_0/checkpoints/epoch=7-step=24000.ckpt')\n",
    "model = ImageClassifierModel.load_from_checkpoint(mp, encoder=enc, classifier=clf, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the test data\n",
    "trainer.test(model=model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alzheimer MRI Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will build on our knowledge of PT and PTL to create a model that classifies input MRI images of a patient's brain to infer the severity of dementia. This is a multi-class problem where the goal is to differentiate between:\n",
    "1. Non Demented\n",
    "2. Very Mild Dementia\n",
    "3. Mild Dementia\n",
    "4. Moderate Dementia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader\n",
    "This dataset is not provided in the PT library. As such, we will need to develop custom DataLoaders to handle the data. We will extend the PT [LightningDataModule](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningDataModule.html#lightning.pytorch.core.LightningDataModule) which will handle much of the work for us. Also, because our data is organized in a directory where the subdirectories are arranged by outcome label, we will use the [torchvision ImageFolder](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html#torchvision.datasets.ImageFolder) to manage the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlzheimerDataModule(L.LightningDataModule):\n",
    "    def __init__(self, root_dir, transform=None, batch_size=16):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.batch_size = batch_size\n",
    "        if transform is None:\n",
    "            self.transform=transforms.Compose([\n",
    "                      transforms.RandomRotation(10),      # rotate +/- 10 degrees\n",
    "                      transforms.RandomHorizontalFlip(),  # reverse 50% of images\n",
    "                      transforms.ToTensor()])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "        self.classes = sorted(os.listdir(self.root_dir))\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = datasets.ImageFolder(root=self.root_dir, transform=self.transform)\n",
    "        n_data = len(dataset)\n",
    "        n_train = int(0.8 * n_data)\n",
    "        n_test = n_data - n_train\n",
    "\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(dataset, [n_train, n_test])\n",
    "\n",
    "        train_size = int(0.9 * len(train_dataset))\n",
    "        val_size = len(train_dataset) - train_size\n",
    "        train_dataset, val_dataset = utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "        self._train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self._val_dataloader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "        self._test_dataloader = DataLoader(test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._test_dataloader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self._val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create variables for our dataloaders for model training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(rs)\n",
    "alzheimers_dm = AlzheimerDataModule(root_dir=os.path.join(dir_dataroot, 'alzheimer-mri', 'Dataset'), batch_size=32)\n",
    "alzheimers_dm.setup()\n",
    "alzheimers_train_dataloader=alzheimers_dm.train_dataloader()\n",
    "alzheimers_val_dataloader=alzheimers_dm.val_dataloader()\n",
    "alzheimers_test_dataloader=alzheimers_dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's visualize a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(alzheimers_train_dataloader))\n",
    "imgs = batch[0]\n",
    "labels = batch[1]\n",
    "grid = make_grid(imgs)\n",
    "print(\"Batch Dimensions:\", imgs.shape)\n",
    "print(\"Batch Targets:\",labels)\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alzheimers Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 (3 points)\n",
    "Define new Image Encoder and Classifier modules for the MRI data. Specifically, in the code cells below adapt the `__init__` methods from `ImageEncoder` and `Classifier` classes we created for the FashionMNIST data by changing the number of kernels in the two conv2d layers to 32 and 64 respectively and adjust for the number of input channels for the first conv2d layer which is now 3 for the mri images. Hint, you can use the output shape from the `MRIImageEncoder` (first cell) to determine the appropriate input size for the linear layer in the `MRIClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        ##### START YOUR CODE HERE #####\n",
    "        pass\n",
    "        ##### END YOUR CODE HERE #####\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "# Test the ImageEncoder with random input data\n",
    "encoder = MRIImageEncoder()\n",
    "input_image = torch.randn(1, 3, 128, 128)  # batch_size x channels x height x width\n",
    "output_features = encoder(input_image)\n",
    "print(\"Output shape:\", output_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        ##### START YOUR CODE HERE #####\n",
    "        pass\n",
    "        ##### END YOUR CODE HERE #####\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)  # Flatten the input tensor\n",
    "        x = self.linear(x)   # Pass through the linear layer\n",
    "        return x\n",
    "\n",
    "# Test the module with random input data\n",
    "module = MRIClassifier()\n",
    "input_tensor = torch.randn(5, 64, 31, 31)  # Batch size of 5, input tensor shape [5, 32, 7, 7]\n",
    "output = module(input_tensor)\n",
    "print(\"Output shape:\", output.shape)  # Expected output shape: [5, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem (2 points)\n",
    "We always want to make our code as resusable as possible. What change could you make to the `init` method in `ImageEncoder` and the `Classifier` classes to allow them to be resued without creating new classes for the MRI data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 (1 point)\n",
    "In the code cell below, train the MRI model using a PTL Trainer (as shown with the FashionMNIST model above). However, when constructing the `Trainer` object, set `max_epochs=20`. Be sure to pass the correct training and validation loaders (alzheimers_val_dataloader and alzheimers_test_dataloader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(rs)\n",
    "encoder = MRIImageEncoder()\n",
    "classifier = MRIClassifier()\n",
    "mri_model = ImageClassifierModel(encoder, classifier, 4)\n",
    "\n",
    "##### START YOUR CODE HERE #####\n",
    "trainer = None\n",
    "##### END YOUR CODE HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Set Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca = mri_model.val_metrics_tracker.compute_all()['MulticlassAccuracy']\n",
    "plt.plot(range(1, len(mca)+1), mca, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Epoch Validation Accuracy')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=mri_model, dataloaders=alzheimers_dm.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = mri_model.test_metrics_tracker.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = sns.heatmap(rslt['MulticlassConfusionMatrix'], annot=True, fmt='d', cmap='Blues')\n",
    "cmp.set_xlabel('Predicted label')\n",
    "cmp.set_xticklabels(alzheimers_dm.classes, rotation=45)\n",
    "cmp.set_yticklabels(alzheimers_dm.classes, rotation=0)\n",
    "cmp.set_ylabel('Actual label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = rslt['MulticlassROC']\n",
    "for i in range(4):\n",
    "    plt.plot(fpr[i], tpr[i], label=alzheimers_dm.classes[i])\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learing\n",
    "In many cases, especially when we have limted data, we can use transfer learning where a more complex model, trained on a very large dataset from another domain is used fine tuned to a new problem. Here, we demonstrate transfer learning on the MRI problem. Specifically, we use the [SqueezeNet](https://pytorch.org/vision/stable/models/generated/torchvision.models.squeezenet1_1.html#torchvision.models.SqueezeNet1_1_Weights) model which was trained on ImageNet data as our encoder. The SqueezeNet model is deep model with many CNN layers that was trained to classify images from ImageNet. We will only use the the \"encoder\" portion of the SqueezeNet model (contained in the `features` variable of the SqueezeNet model). We will add additional layers to serve as our classifier. When we train the model, we will only update the classifier parameters, though we could also update the encoder parameters if we had sufficient data and computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's recreate our data module as we will utilize additional transforms on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(rs)\n",
    "transform=transforms.Compose([\n",
    "        transforms.RandomRotation(10),      # rotate +/- 10 degrees\n",
    "        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n",
    "        transforms.Resize(224),             # resize shortest side to 224 pixels\n",
    "        transforms.CenterCrop(224),         # crop longest side to 224 pixels at center\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "        \n",
    "])\n",
    "\n",
    "alzheimers_dm = AlzheimerDataModule(root_dir=os.path.join(dir_dataroot, 'alzheimer-mri', 'Dataset'), batch_size=32, transform=transform)\n",
    "alzheimers_dm.setup()\n",
    "alzheimers_train_dataloader=alzheimers_dm.train_dataloader()\n",
    "alzheimers_val_dataloader=alzheimers_dm.val_dataloader()\n",
    "alzheimers_test_dataloader=alzheimers_dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the `SqueezeNetMRIImageEncoder` using the `SqueezeNet` features backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeNetMRIImageEncoder(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        backbone = models.squeezenet1_1(weights=models.SqueezeNet1_1_Weights.DEFAULT)\n",
    "        self.encoder = backbone.features\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "# Test the ImageEncoder with random input data\n",
    "encoder = SqueezeNetMRIImageEncoder()\n",
    "input_image = torch.randn(1, 3, 224, 224)  # batch_size x channels x height x width\n",
    "output_features = encoder(input_image)\n",
    "print(\"Output shape:\", output_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the classifier which is slightly more complicate than the one we created previously as we want to add a final convolutional layer as was done with the original SqueezeNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeNetMRIClassifierSquezeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.num_classes = 4\n",
    "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3), final_conv, nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)   # Pass through the linear layer\n",
    "        return x.squeeze()\n",
    "\n",
    "# Test the module with random input data\n",
    "module = SqueezeNetMRIClassifierSquezeNet()\n",
    "input_tensor = torch.randn(5, 512, 13, 13)  # Batch size of 5, input tensor shape [5, 32, 7, 7]\n",
    "output = module(input_tensor)\n",
    "print(\"Output shape:\", output.shape)  # Expected output shape: [5, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to create our overall model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifierSqueezeNet(ImageClassifierModel):\n",
    "    def __init__(self, encoder, classifier, num_classes, freeze_encoder=True):\n",
    "        super().__init__(encoder, classifier, num_classes)\n",
    "        if freeze_encoder:\n",
    "            self.encoder.freeze()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.classifier.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can train and evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(rs)\n",
    "encoder = SqueezeNetMRIImageEncoder()\n",
    "classifier = SqueezeNetMRIClassifierSquezeNet()\n",
    "mri_squeeze_model = ImageClassifierSqueezeNet(encoder, classifier, 4)\n",
    "\n",
    "trainer = L.Trainer(default_root_dir=dir_lightning, \n",
    "                    max_epochs=50,\n",
    "                    callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)],\n",
    "                    accelerator='gpu')\n",
    "trainer.fit(model=mri_squeeze_model, train_dataloaders=alzheimers_dm.train_dataloader(), val_dataloaders=alzheimers_dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mca = mri_squeeze_model.val_metrics_tracker.compute_all()['MulticlassAccuracy']\n",
    "plt.plot(range(1, len(mca)+1), mca, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Epoch Validation Accuracy')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=mri_squeeze_model, dataloaders=alzheimers_dm.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = mri_squeeze_model.test_metrics_tracker.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp = sns.heatmap(rslt['MulticlassConfusionMatrix'], annot=True, fmt='d', cmap='Blues')\n",
    "cmp.set_xlabel('Predicted label')\n",
    "cmp.set_ylabel('Actual label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = rslt['MulticlassROC']\n",
    "class_map = {0: 'Mild', 1: 'Moderate', 2: 'Healthy', 3: 'Very Mild'}\n",
    "for i in range(4):\n",
    "    plt.plot(fpr[i], tpr[i], label=class_map[i])\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")   #\"cuda:0\"\n",
    "class_names=sorted(os.listdir(alzheimers_dm.root_dir))\n",
    "mri_squeeze_model.eval()\n",
    "y_true=[]\n",
    "y_pred=[]\n",
    "with torch.no_grad():\n",
    "    for test_data in alzheimers_dm.test_dataloader():\n",
    "        test_images, test_labels = test_data[0].to(device), test_data[1].to(device)\n",
    "        pred = mri_squeeze_model(test_images).argmax(dim=1)\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())\n",
    "\n",
    "print(classification_report(y_true,y_pred,target_names=class_names,digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
